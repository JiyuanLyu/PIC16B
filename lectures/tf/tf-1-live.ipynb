{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:PIC16B] *",
      "language": "python",
      "name": "conda-env-PIC16B-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "tf-1-live.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-1-live.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swiss-undergraduate"
      },
      "source": [
        "# Advanced Machine Learning with TensorFlow\n",
        "\n",
        "In this lecture, we'll begin a relatively long arc in which we will learn to use the TensorFlow package for advanced machine learning, with an emphasis on neural networks.  \n",
        "\n",
        "## A Few Notes on Google and Ethics\n",
        "\n",
        "[TensorFlow](https://www.tensorflow.org/) is a Google product. By teaching you TensorFlow, I take a small step toward extending the influence of Google in the field of machine learning and in the STEM community more broadly. By using TensorFlow in and out of this class, you will do the same. It's important that we both go into this with open eyes about some of the ethical questions surrounding Google's recent work in machine learning. I'd like to stress that I am not an expert on ethics in artificial intelligence. There are likely MANY other important ethical concerns about Google's work  (and the work of other giants in tech and artificial intelligence, like Facebook and Amazon) which are not on my radar. \n",
        "\n",
        "### 1. Language Models and Dr. Timnit Gebru\n",
        "\n",
        "In December 2020, Google [fired](https://www.washingtonpost.com/technology/2020/12/03/timnit-gebru-google-fired/) prominent AI ethicist Dr. Timnit Gebru, an Ethiopian-American woman, over a sequence of events surrounding [one of her papers](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf). In this paper, Gebru and coauthors raise ethical concerns about language models---like the ones that Google uses for predictive text applications. These concerns include:\n",
        "\n",
        "- The role of these models in homogenizing online culture. \n",
        "- The environmental cost of training these (in some cases, comparable to the carbon footprint of a transatlantic flight). \n",
        "- The frequent instances of language models learning to reproduce biased or hateful text. \n",
        "\n",
        "Dr. Gebru and her collaborators ultimately recommend a more thoughtful, \"small\", and ethically-informed approach to constructing language models. Naturally, this recommendation would require extensive reorganization of one of Google's major research areas. Possibly because they felt threatened by this and related recommendations, Google managers invented an additional, internal review layer for this paper, even after it had already been accepted at a prominent computer science conference. Dr. Gebru's protest of this made-up red tape was one of the events that led to her firing. \n",
        "\n",
        "Dr. Gebru's colleague Margaret Mitchell was [also recently fired](https://www.zdnet.com/article/google-fires-top-ethical-ai-expert-margaret-mitchell/) in the fall-out from this incident. \n",
        "\n",
        "### 2. Google Translate Is Sexist\n",
        "\n",
        "It is well-documented that machine learning algorithms trained on natural text can inherit biases present in those texts. One of the most direct ways in which we can observe such bias is in Google Translate. Some languages, such as Hungarian, do not possess gendered pronouns. When Google Translate attempts to render these pronouns into a gendered language like English, assumptions are made, as pointed out in [this Tweet by Dora Vargha](https://twitter.com/DoraVargha/status/1373211762108076034?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1373211762108076034%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fd-7356743851859968838.ampproject.net%2F2103240330002%2Fframe.html).  Let's demonstrate with the following English sentences. \n",
        "\n",
        "> **he** cooks.\n",
        "> **she** is a political leader.\n",
        "> **she** is an engineer.\n",
        "> **he** is a cleaner.\n",
        "> **he** is beautiful. \n",
        "> **she** is strong. \n",
        "\n",
        "Translate these into Hungarian and back via Google Translate, and here's what you'll get: \n",
        "\n",
        "> **she** cooks.\n",
        "> **he** is a political leader.\n",
        "> **he** is an engineer.\n",
        "> **she** is cleaning.\n",
        "> **she** is beautiful.\n",
        "> **he** is strong.\n",
        "\n",
        "Considering that English *has* a gender neutral pronoun (*they*), this would be an easy item to fix, which Google has thus far declined to do. \n",
        "\n",
        "### 3. Historical Racial and Gender Biases\n",
        "\n",
        "Google Search has a striking history of bias against Black people, especially Black women. This bias was made widely public by UCLA professor Safiya Noble in her book *Algorithms of Oppression*. In one of Dr. Nobel's most famous examples, top results for the phrase \"black girls\" in 2011 consisted of links to porn sites, which did not hold true of searches for \"white girls\" or \"black men.\" As late as 2016, an image search for \"gorillas\" would surface pictures of Black individuals. You can find a brief synopsis of some of Dr. Noble's findings [here](https://time.com/5209144/google-search-engine-algorithm-bias-racism/) (content warning: highly sexually explicit language).  Google has since taken steps to improve these specific examples. "
      ],
      "id": "swiss-undergraduate"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "manufactured-reproduction"
      },
      "source": [
        "Keeping these items in mind, let's now begin our study of machine learning via Tensorflow. \n",
        "\n",
        "## Tensors\n",
        "\n",
        "So, uh, what's a tensor? As you may remember from Riemannian geometry, a prerequisite for this class, \n",
        "\n",
        "> An $s$-contravariant and $t$-covariant *tensor* $\\sigma$ on a vector space $V$ is a multilinear map $\\sigma: \\left(V^*\\right)^s \\times V^t \\rightarrow \\mathbb{R}$, where $V^*$ denotes the space of linear functions on $V$...  \n",
        "\n",
        "<br> <br>\n",
        "\n",
        "Just kidding! A tensor is pretty much just a Numpy array. \n",
        "\n",
        "<figure class=\"image\" style=\"width:40%\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/tensor.jpeg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Here's another one in case that one didn't sink in: \n",
        "\n",
        "<figure class=\"image\" style=\"width:40%\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/tensor-2.jpeg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Most of the time, we'll work with 2-dimensional tensors (matrices), with the occasional 3-dimensional tensor thrown in for spice. "
      ],
      "id": "manufactured-reproduction"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assisted-style"
      },
      "source": [
        "## Structure of Neural Networks\n",
        "\n",
        "Before we start working with TensorFlow, I'd like to build up our understanding of what a neural network actually does to data. We'll see that neural networks are built up of very simple mathematical transformations, which are then stacked on top of each other to produce complex models. \n",
        "\n",
        "Suppose I have a data point $x$, with 5 features (columns): "
      ],
      "id": "assisted-style"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "identified-simple"
      },
      "source": [
        "import numpy as np\n",
        "x = np.random.rand(5)\n",
        "x"
      ],
      "id": "identified-simple",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "natural-carbon"
      },
      "source": [
        "A single *unit* of a neural network has two stages: \n",
        "\n",
        "1. The unit takes the entries of $x$, multiplies them by some *weights*, and adds them together, forming the linear combination $y = \\sum_i w_i x_i$. This number $y$ is often called the *activation*. \n",
        "2. Second, the unit hits $y$ with a non-linear function, producing a new output $z$: $z = g(y)$. \n",
        "\n",
        "A common nonlinear function is the sigmoid (same function as used in logistic regression): "
      ],
      "id": "natural-carbon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "labeled-plant"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "x_space = np.linspace(-10, 10)\n",
        "plt.plot(x_space, sigmoid(x_space))"
      ],
      "id": "labeled-plant",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atomic-management"
      },
      "source": [
        "So, given a weight vector $w$, we can easily compute the layer output  for $x$, remembering that `w @ x` will calculate the inner product for us. "
      ],
      "id": "atomic-management"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ancient-schema"
      },
      "source": [
        "w = np.random.rand(5)\n",
        "z = sigmoid(w@x)\n",
        "z"
      ],
      "id": "ancient-schema",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seven-council"
      },
      "source": [
        "In practice, we perform this computation using many data points and many units simultaneously. In this case, we can represent the data points as an $m \\times n$ matrix $\\mathbf{X}$, where $m$ is the number of data points and $n$ is the number of columns. We can represent the matrix of weights as an $p \\times n$ matrix $\\mathbf{W}$, where $p$ is the number of units in the layer. We can then compute all the activations simultaneously by computing the matrix product $\\mathbf{Y} = \\mathbf{W} \\mathbf{X}^T$. \n",
        "\n",
        "Here's a nice picture from the [website of Jeremy Jordan](https://www.jeremyjordan.me/intro-to-neural-networks/) to visualize what's going on. In this diagram, `a` is the activation (what I've been calling $y$). \n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2017-11-07-at-12.53.07-PM.png)\n",
        "\n",
        "We can then hit $\\mathbf{Y}$ with the sigmoid function (entrywise) to form the output $\\mathbf{Z}$. "
      ],
      "id": "seven-council"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "earned-little"
      },
      "source": [
        "m = 100\n",
        "n = 5\n",
        "\n",
        "p = 10\n",
        "\n",
        "X = np.random.rand(m, n)\n",
        "\n",
        "W = 1/p * np.random.rand(p, n)\n",
        "\n",
        "new_X = sigmoid(W @ X.T)\n",
        "\n",
        "plt.imshow(new_X)"
      ],
      "id": "earned-little",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "champion-rings"
      },
      "source": [
        "Here's the thing: we now have a *new* matrix, that I'll call $\\mathbf{X}_1$, of size $p \\times m$. We can now plug that into a NEW layer, getting a new matrix $\\mathbf{X}_2$. We can plug THAT into a new layer, etc. By composing all these operations, we get a single, multistage function. **This is a deep neural network.** At the end of the pipeline, we should make sure we have an output that corresponds to the task we want to perform. For example, if we want to do binary classification (classification between two categories), we our weights to look something like this: "
      ],
      "id": "champion-rings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prospective-geology"
      },
      "source": [
        ""
      ],
      "id": "prospective-geology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insured-dover"
      },
      "source": [
        "The dimension of size 2 holds numbers describing the model's confidence in each of the two categories. At the moment, of course, we haven't done anything like fitting our model to data, so these predictions aren't meaningful (yet).\n",
        "\n",
        "Note that, while we don't have any control over the data $\\mathbf{X}$, we are free to choose the weights $\\mathbf{W}$ however we please. Traiing a neural network corresponds to the task of choosing these weights in such a way as to appropriately match the data. \n",
        "\n",
        "In summary, at the core of neural networks are the operations of (a) matrix multiplication and (b) simple, nonlinear functions. These are stacked on top of each other (composed) to produce sophisticated models. Naturally, we are not going to do all our bookkeeping of weights and dimensions and whatnot by hand. Managing this bookkeeping is precisely what TensorFlow is for. "
      ],
      "id": "insured-dover"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "popular-sensitivity"
      },
      "source": [
        "## TensorFlow Basics"
      ],
      "id": "popular-sensitivity"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worldwide-universe"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "id": "worldwide-universe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "third-association"
      },
      "source": [
        "We can create a simple, \"constant\" tensor using `tf.constant`: "
      ],
      "id": "third-association"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miniature-outdoors"
      },
      "source": [
        ""
      ],
      "id": "miniature-outdoors",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "danish-gathering"
      },
      "source": [
        "As you can see from the output, this object has a `shape`, a `dtype`, and even an internal `numpy` representation -- it really is a lot like a `numpy` array! Like `numpy` arrays, we can do various mathematical operations: "
      ],
      "id": "danish-gathering"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buried-affiliate"
      },
      "source": [
        ""
      ],
      "id": "buried-affiliate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "about-curve"
      },
      "source": [
        ""
      ],
      "id": "about-curve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opponent-findings"
      },
      "source": [
        "There *is* a reason that we use `tf.Tensor` rather than Numpy array objects. One of the primary reasons is that the tensor data type is set up to support *automatic differentiation*, which is important when it comes time to train our models. That said, it's sometimes useful to convert back to literal Numpy arrays, which can usually be done like this: "
      ],
      "id": "opponent-findings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retained-guyana"
      },
      "source": [
        ""
      ],
      "id": "retained-guyana",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "structural-alliance"
      },
      "source": [
        "Really, they could have called the whole thing ArrayFlow or RectanglesOfNumbersFlow, but those don't really say \"I am a very smart person\" in quite the same way. \n",
        "\n",
        "### Dtypes\n",
        "\n",
        "Because computation on 64-bit floating point numbers can be quite expensive at scale, most operations in TensorFlow prefer that you supply floating point numbers with data type `float32`. For example: "
      ],
      "id": "structural-alliance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amateur-wednesday"
      },
      "source": [
        ""
      ],
      "id": "amateur-wednesday",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appreciated-robertson"
      },
      "source": [
        "Supplying `float64` data types will usually lead to annoying warning messages, and possibly slower performance. \n",
        "\n",
        "While it's good to know what tensors are and how they work, we usually won't need to construct them explicitly. Rather, we'll be able to feed Numpy arrays to our models, which will handle all the tensor operations internally. "
      ],
      "id": "appreciated-robertson"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "consolidated-pasta"
      },
      "source": [
        "## Layers\n",
        "\n",
        "*Layers* are the building blocks of models. You can think of a layer as a function that takes in one tensor and spits out another tensor, possibly of a different shape. Many layers have requirements on the kinds of tensors they admit; for example, they might only work on 2d tensors.\n",
        "\n",
        "In this course, we'll always work with layers via the high-level Keras API, which allows us to easily create and combine layers. "
      ],
      "id": "consolidated-pasta"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swedish-teens"
      },
      "source": [
        ""
      ],
      "id": "swedish-teens",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loaded-likelihood"
      },
      "source": [
        ""
      ],
      "id": "loaded-likelihood",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "possible-service"
      },
      "source": [
        ""
      ],
      "id": "possible-service",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "catholic-ticket"
      },
      "source": [
        "Now we can create a layer. Note that we first have to *make* the layer before we *call* the layer on any data. The `Dense` layer is the simplest and among the most generally useful kinds of layers, and corresponds fairly directly to the computations that we described \"by hand\" above. "
      ],
      "id": "catholic-ticket"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "combined-victorian"
      },
      "source": [
        ""
      ],
      "id": "combined-victorian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "promising-navigation"
      },
      "source": [
        "The `units` argument controls the shape of the output. The `Dense` layer interprets an `m` $\\times$ `n` tensor as a set of data points with `m` rows and `n` columns (or features). It outputs a new tensor with `m` rows and `units` columns. So, you can think of `units` as controlling the number of *hidden features* learned by this layer. \n",
        "\n",
        "What's the deal with the actual outputs? Well, as we saw above, this layer has parameters (*weights*) which control how these numbers are calculated. Because we haven't done any model training, these weights are pretty much random, so the output is random as well. We'll look at training the model in the next section. "
      ],
      "id": "promising-navigation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swiss-justice"
      },
      "source": [
        "## Models\n",
        "\n",
        "A model consists of: \n",
        "\n",
        "- A sequence of layers. The final layer is an *output* layer, and plays a major role in determining what \"kind\" of model we have (e.g. regression vs. classification). \n",
        "- Specs for how the model should be fit to data. The most important choice to make here is the *loss function*, which governs how, mathematically, the model will be judged on its performance.\n",
        "\n",
        "Let's download some real data, create a model, and evaluate its performance. "
      ],
      "id": "swiss-justice"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noble-northern"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# #BestData\n",
        "url = \"https://philchodrow.github.io/PIC16A/datasets/palmer_penguins.csv\"\n",
        "penguins = pd.read_csv(url)\n",
        "\n",
        "# only use these columns\n",
        "df = penguins[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\",  \"Species\"]]\n",
        "df = df.dropna()\n",
        "\n",
        "# categorically encode the \"Species\" column\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df[\"Species\"])\n",
        "\n",
        "# predictor data\n",
        "X = df[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\"]]\n",
        "\n",
        "# convert to numpy array\n",
        "X = np.array(X, dtype = np.float32)\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
      ],
      "id": "noble-northern",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "little-privilege"
      },
      "source": [
        ""
      ],
      "id": "little-privilege",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dietary-raleigh"
      },
      "source": [
        ""
      ],
      "id": "dietary-raleigh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggregate-radio"
      },
      "source": [
        "The simplest way to make a model is by using the `tf.keras.models.Sequential` API, which allows you to construct a model by simply passing a list of layers. Let's do two \"hidden\" layers of 500 units each, and then an output layer of 3 units (corresponding to 3 penguin species). If you've used the [multilayer perceptron algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) from `scikit-learn` before, the model below is essentially a multilayer perceptron with parameter `hidden_layer_sizes = (500, 500)`. \n",
        "\n",
        "Specifying the `input_shape` in the first layer is not actually necessary unless you want to see the `summary` below. Otherwise, the shape will be inferred the first time you fit the model. "
      ],
      "id": "aggregate-radio"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "indirect-decline"
      },
      "source": [
        ""
      ],
      "id": "indirect-decline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flexible-mills"
      },
      "source": [
        "This `input_shape` specifies that there are an undetermined number of rows and 3 columns in the input data. \n",
        "\n",
        "The reason we've chosen the final layer to output 3 features is that there are 3 species in the data. \n",
        "\n",
        "The `model.summary()` is a nice way to get an overview of how your model operates. "
      ],
      "id": "flexible-mills"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "specialized-anime"
      },
      "source": [
        ""
      ],
      "id": "specialized-anime",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aquatic-crisis"
      },
      "source": [
        "Wow -- 250K parameters in this model! Large numbers of parameters allow your model to learn more complex relationships in your data, but also take longer to train and can lead to overfitting. Checking your model summary is usually a good idea, as it can help you get a feeling for the training time required for your model. "
      ],
      "id": "aquatic-crisis"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innovative-peeing"
      },
      "source": [
        "Let's make some \"predictions\":"
      ],
      "id": "innovative-peeing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celtic-staff"
      },
      "source": [
        ""
      ],
      "id": "celtic-staff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "distant-script"
      },
      "source": [
        "These are predictions for the first 5 rows of the data set. Each row is a specific penguin, and each column is one of the three species. The number in the column is related to the model's guess about the species. To convert these into probabilities, we use the `Softmax` layer: "
      ],
      "id": "distant-script"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tropical-direction"
      },
      "source": [
        ""
      ],
      "id": "tropical-direction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connected-exhaust"
      },
      "source": [
        "The model hasn't been trained yet, so these \"predictions\" don't really mean anything. "
      ],
      "id": "connected-exhaust"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyrighted-journey"
      },
      "source": [
        "### Model Training\n",
        "\n",
        "Training has two stages. First we *compile* the model, by specifying the loss function and optimization algorithm. Then, we perform the actual training. The choice of loss function is highly dependent on your problem domain. For classification problems, categorical cross-entropy is a good one.  Finally, the `metrics` argument is helpful for controlling which model performance measures are shown when training or evaluating the model. "
      ],
      "id": "copyrighted-journey"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "herbal-replacement"
      },
      "source": [
        ""
      ],
      "id": "herbal-replacement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vocational-coaching"
      },
      "source": [
        "Finally ready for training! "
      ],
      "id": "vocational-coaching"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exclusive-afghanistan"
      },
      "source": [
        ""
      ],
      "id": "exclusive-afghanistan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "italic-progress"
      },
      "source": [
        "Now we can evaluate the model on our test data: "
      ],
      "id": "italic-progress"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "based-theme"
      },
      "source": [
        ""
      ],
      "id": "based-theme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checked-badge"
      },
      "source": [
        "Our model is able to correctly predict the species of a penguin based on its culmen length, culmen depth, and flipper length over 95% of the time. Not bad! Further training (by choosing more `epochs` in `model.fit` could potentially improve this further---or lead to overfitting. \n",
        "\n",
        "### Prediction Probabilities\n",
        "\n",
        "A minor annoyance is that, even after training, our model still doesn't have very interpretable outputs: "
      ],
      "id": "checked-badge"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "substantial-syria"
      },
      "source": [
        ""
      ],
      "id": "substantial-syria",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "streaming-india"
      },
      "source": [
        "Having trained our model, we can create a new, interpretable version by adding a Softmax layer. "
      ],
      "id": "streaming-india"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outside-gabriel"
      },
      "source": [
        ""
      ],
      "id": "outside-gabriel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ready-norfolk"
      },
      "source": [
        ""
      ],
      "id": "ready-norfolk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unavailable-equity"
      },
      "source": [
        "Each row now reflects the model's level of confidence that the given penguin is of the given species, which is much more interpretable. \n",
        "\n",
        "(*In case you're wondering, we don't include the Softmax layer in the model before we train it for numerical reasons*). "
      ],
      "id": "unavailable-equity"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "banned-packing"
      },
      "source": [
        "## What's next? \n",
        "\n",
        "We've learned about tensors, layers, and models, and we've used a simple model to make predictions on a pretty small data set. In coming lectures, we'll ask questions like: \n",
        "\n",
        "- How can I represent text or images as tensors? \n",
        "- How can I perform classification, regression, or clustering tasks? \n",
        "- How can I interpret what my model is doing? \n",
        "- How can I speed up my model training? "
      ],
      "id": "banned-packing"
    }
  ]
}