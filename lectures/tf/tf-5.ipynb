{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-knight",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-timber",
   "metadata": {
    "id": "compound-switch"
   },
   "source": [
    "# Text Generation with Recurrent Neural Networks\n",
    "\n",
    "In this set of lecture notes, we'll consider a new kind of machine learning task. Previously, we've focused on *classification* problems. In classification problems, the goal is to assign a given piece of data to one of several categories. Today, we'll instead consider a simple  *generation* problem. A *generative* model can be used to create \"realistic\" examples after it's been trained. Generative models are at the heart of machine learning topics like deepfakes, language modeling, and [style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer).  \n",
    "\n",
    "\n",
    "\n",
    "*Parts of these lecture notes were based on [this tutorial](https://keras.io/examples/generative/lstm_character_level_text_generation/). It is recommended to run the code contained in these notes in a Google Colab instance with GPU acceleration enabled.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genuine-monster",
   "metadata": {
    "id": "august-syracuse"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-civilian",
   "metadata": {
    "id": "VO91nEUSzWtO"
   },
   "source": [
    "\n",
    "## Our Task\n",
    "\n",
    "Today, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of *Star Trek: Deep Space Nine.*\n",
    "\n",
    "<figure class=\"image\" style=\"width:300px\">\n",
    "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/DS9.jpg\" alt=\"\">\n",
    "  <figcaption><i></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "In particular, we are going to attempt to teach a neural  network to generate *episode scripts*. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "later-genre",
   "metadata": {
    "id": "jOJ-RVVNp4M0"
   },
   "outputs": [],
   "source": [
    "start_episode = 20 # Start in Season 2, Season 1 is not very good\n",
    "num_episodes = 50  # only pick this many episodes to train on\n",
    "\n",
    "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
    "star_trek_scripts = pd.read_json(url)\n",
    "\n",
    "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
    "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
    "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
    "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
    "    text = text.replace(char, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "returning-bible",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdzuQ2AoqP63",
    "outputId": "111bf21e-e5d7-41c5-e4cf-0d7ad4d6352d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Last\n",
      "time on Deep Space Nine.  \n",
      "SISKO: This is the emblem of the Alliance for Global Unity. They call\n",
      "themselves the Circle. \n",
      "O'BRIEN: What gives them the right to mess up our station? \n",
      "ODO: They're an extremist faction who believe in Bajor for the\n",
      "Bajorans. \n",
      "SISKO: I can't loan you a Starfleet runabout without knowing where you\n",
      "plan on taking it. \n",
      "KIRA: To Cardassia Four to rescue a Bajoran prisoner of war. \n",
      "(The prisoners are rescued.) \n",
      "KIRA: Come on. We have a ship waiting. \n",
      "JARO: What you \n"
     ]
    }
   ],
   "source": [
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-kentucky",
   "metadata": {
    "id": "icYyItL_1Qc3"
   },
   "source": [
    "Our first step, as usual, is data preparation. What we need to do is format the data in such a way that we can treat the situation as a classification problem after all. That is: \n",
    "\n",
    "> Given a string of text, predict the next character in that string. \n",
    "\n",
    "Doing this repeatedly will allow the model to generate large bodies of text. \n",
    "\n",
    "To do this, we want to split our data like so: \n",
    "\n",
    "```\n",
    "predictor = \"to boldly g\"\n",
    "target    = \"o\"\n",
    "```\n",
    "\n",
    "The following function will do this for us. The `max_len` argument gives the number of characters that should be in the predictor string, and the `step_size` argument lets us skip indices if we want to in order to decrease the size of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-grass",
   "metadata": {
    "id": "abroad-hypothesis"
   },
   "outputs": [],
   "source": [
    "def split(raw_text, max_len, step_size = 1):\n",
    "\n",
    "    lines = []\n",
    "    next_chars = []\n",
    "\n",
    "    for i in range(0, len(text) - max_len, step_size):\n",
    "        lines.append(text[i:i+max_len])\n",
    "        next_chars.append(text[i+max_len])\n",
    "    \n",
    "    return lines, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "difficult-locator",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "distributed-performance",
    "outputId": "8aa05880-38d7-4a47-d70a-fee877e7fad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he emblem of the All     =>    i\n",
      "blem of the Alliance     =>     \n",
      "of the Alliance for      =>    G\n",
      "e Alliance for Globa     =>    l\n",
      "iance for Global Uni     =>    t\n"
     ]
    }
   ],
   "source": [
    "max_len = 20\n",
    "\n",
    "lines, next_chars =  split(text, max_len = max_len, step_size = 5)\n",
    "for i in range(10, 15):\n",
    "    print(lines[i] + \"     =>    \" + next_chars[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-faith",
   "metadata": {
    "id": "ojwSL8eX2F2p"
   },
   "source": [
    "Our next step is to vectorize the characters. This is similar to the word vectorization task, but it's simple enough in this case that's arguably more convenient to actually handle it outside of TensorFlow. It is also possible to handle vectorization using TensorFlow tools, as demonstrated in [this tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anonymous-wiring",
   "metadata": {
    "id": "gSaFo1c88XQS"
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "char_indices = {char : chars.index(char) for char in chars}\n",
    "X = np.zeros((len(lines), max_len, len(chars)))\n",
    "y = np.zeros((len(lines), 1), dtype = np.int32)\n",
    "for i, line in enumerate(lines):\n",
    "\tfor t, char in enumerate(line):\n",
    "\t\tX[i, t, char_indices[char]] = 1\n",
    "\ty[i] = char_indices[next_chars[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-auditor",
   "metadata": {
    "id": "O0fmER3H-KSn"
   },
   "source": [
    "Let's take a look at what happened here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bigger-officer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MJqyOgI4cjr",
    "outputId": "9ab4a48e-ed9f-480a-aa28-37e39d7a2c25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((314163, 20, 78), (314163, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sapphire-spanking",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5vN0_A3-M8y",
    "outputId": "9567476a-ee61-4315-b875-595722fd97b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adult-roberts",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OgFmGrG-UkM",
    "outputId": "816b7aff-48d6-46e1-8251-31da9fc09be4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-advice",
   "metadata": {
    "id": "E6fgVac-4fze"
   },
   "source": [
    "Now we're ready to perform a train-test split: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "approximate-puzzle",
   "metadata": {
    "id": "jppzOG0uV_6i"
   },
   "outputs": [],
   "source": [
    "train_len = int(0.7*X.shape[0])\n",
    "X_train = X[0:train_len]\n",
    "X_val = X[train_len:]\n",
    "\n",
    "y_train = y[0:train_len]\n",
    "y_val  = y[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-china",
   "metadata": {
    "id": "YjLMfcZE4jgX"
   },
   "source": [
    "Model time! We'll use a simple *Long Short-Term Memory* (LSTM) model for this example. LSTMs are one example of *recurrent* neural network layers. Here's a diagram illustrating the schematic functioning of a recurrent layer. \n",
    "\n",
    "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "*Image credit: [Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), OpenAI*\n",
    "\n",
    "On the lefthand side, we have a \"zoomed out\" picture of a recurrent neural network layer. On the righthand side, we see the \"zoomed in\" version. The key point here is that output $h_2$ depends not only on input $x_2$, but also, indirectly, on inputs $x_0$ and $x_1$. This means that recurrent neural networks are highly suitable for modeling processes that have temporal structure. Text is an example: the last few characters are the \"history\" of the text. Timeseries data are another clear example, and indeed, we can use a very similar workflow to the one we'll use today in order to do forecasting in timeseries. \n",
    "\n",
    "Since training for this kind of task gets expensive fast, we'll use just one LSTM layer followed by a `Dense` output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "engaged-doctor",
   "metadata": {
    "id": "alive-debut"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    layers.LSTM(128, name = \"LSTM\", input_shape=(max_len, len(chars))),\n",
    "    layers.Dense(len(chars), activation = \"softmax\")        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "connected-manufacturer",
   "metadata": {
    "id": "Un1ValtI9W0R"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "              optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-testimony",
   "metadata": {
    "id": "t_CdRi-L461U"
   },
   "source": [
    "Time for training. We'll do just one epoch for now, mostly just to prove that we've set up our model correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radio-craft",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLzlxzGw9iI5",
    "outputId": "d331c33b-b2f5-4af5-eb59-325ae47195de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 14s 6ms/step - loss: 2.9459 - val_loss: 2.1783\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 2.1127 - val_loss: 1.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3e302110d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code I used to train and save the model\n",
    "# model.fit(X_train, \n",
    "#           y_train,\n",
    "#           validation_data= (X_val, y_val),\n",
    "#           batch_size=128, epochs = 200)\n",
    "# model.save('DS9_model') \n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train,\n",
    "          validation_data= (X_val, y_val),\n",
    "          batch_size=128, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-wrestling",
   "metadata": {
    "id": "Ro1uhnsC5G1r"
   },
   "source": [
    "Rather than training the entire model live during lecture, I'm instead going to load in a saved model that I previously trained for 200 epochs on Google Colab. On Colab, each epoch takes around 10s or so. 200 epochs corresponds to roughly 30 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sophisticated-honduras",
   "metadata": {
    "id": "0IYE6EhM5ZIq"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('DS9_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-melbourne",
   "metadata": {
    "id": "kD7rl_Se5qru"
   },
   "source": [
    "Generative models define *probability distributions* over the space of possible outputs. So, our overall algorithm is going to generate new text in a partially randomized way. To make this happen, we define a `sample` function which will take the model outputs, turn them into probabilities, and then sample from the probabilities to produce a single character (well, technically, an integer corresponding to a single character). \n",
    "\n",
    "An important parameter here is the so-called *temperature* (this terminology comes from statistical physics. When the temperature is high, the model will more frequently choose low-probability characters. This is sometimes interpreted as \"creativity,\" and leads to more unpredictable outputs. When the temperature is low, on the other hand, the model will \"play it safe\" and tend to stick to known patterns. In the extreme limiting case as the temperature approaches 0, the model will ultimately get stuck in \"loops\" in which it repeats common phrases over and over again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bronze-spending",
   "metadata": {
    "id": "bronze-battle"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temp):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    probs = np.exp(preds/temp)\n",
    "    probs = probs / probs.sum()\n",
    "    samp = np.random.multinomial(1, probs, 1)\n",
    "    return np.argmax(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-brisbane",
   "metadata": {
    "id": "E5Gix5AK6b8i"
   },
   "source": [
    "Now that we know how to sample from the model predictions to create a new character, let's now define a convenient function that will allow us to create entire strings of specified length using this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "defensive-yield",
   "metadata": {
    "id": "BvjDjrjvADEf"
   },
   "outputs": [],
   "source": [
    "def generate_string(seed_index, temp, gen_length, model): \n",
    "\n",
    "    gen_seq = np.zeros((max_len + gen_length, len(chars)))\n",
    "    seed = X[seed_index]\n",
    "    gen_seq[0:max_len] = seed\n",
    "    \n",
    "    gen_text = lines[seed_index]\n",
    "\n",
    "    for i in range(0, gen_length):\n",
    "        window = gen_seq[i: i + max_len]\n",
    "        preds = model.predict(np.array([window]))[0]\n",
    "        next_index = sample(preds, temp)\n",
    "        gen_seq[max_len + i, next_index] = True\n",
    "\n",
    "        next_char = chars[next_index]\n",
    "        gen_text += next_char\n",
    "\n",
    "    return(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-franklin",
   "metadata": {
    "id": "fLJ30Flj67PN"
   },
   "source": [
    "Let's try it out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "psychological-pipeline",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCepapOYAmIf",
    "outputId": "7bc98290-f03b-4132-9f2f-6f9075a5b0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "TEMPERATURE: 0.01\n",
      "tioning. \n",
      "KIRA: No p => refitian systems who doend these day have makaly about them. \n",
      "SISKO: Yes. \n",
      "O'BRIEN: Then the sent to the security stapped the station of the same setural schosiaticed to should have and then the station. \n",
      "KIRA: Of course you was a Ferengi go. lo a gay have notring to same then these our back to infexted the transporsely of yours. \n",
      "BASHIR: No, I won't we can trays of the shields a femition. \n",
      "SISKO: I don't know what you're tellod have my eary from the station. \n",
      "KIRA: Of course you would saying is \n",
      "----\n",
      "TEMPERATURE: 0.02\n",
      "tioning. \n",
      "KIRA: No p => reficies and not Commander on have been a meances are mys\n",
      "sure. \n",
      "SISKO: Sinder them is see. \n",
      "SISKO: I don't want to an accure to pating from Nettive secure. \n",
      "KIRA: I'm not going to be a that. \n",
      "ODO: We were sorry whenge to you. I was the station is there a looks like the treaty fame tooura. \n",
      "DAX: I can't be rucking a conside in the station. \n",
      "KIRA: Of course you wouldn't leave me to talk. \n",
      "QUARK: Oh, it make is been coming vesigeral is again. \n",
      "ZEK: I'll look to relequentany. \n",
      "ODO: I stound a means \n",
      "----\n",
      "TEMPERATURE: 0.03\n",
      "tioning. \n",
      "KIRA: No p => rendrans, but they was the station. \n",
      "KIRA: Now about Commander O'Brien. \n",
      "1UGDO: Ill how to anything about armed to know what you can do what he was been a bit of the station pays on the courter back. \n",
      "SISKO: I'm sorry what about your programme and I tell the replicator some like it. \n",
      "SISKO: Let you get them anything when the ship coovisition. \n",
      "KOLOTH: But won't han are to like a try a leady for the sense to the sent life to be a back in probabased to the station was a low at the station. \n",
      "KIRA:  \n",
      "----\n",
      "TEMPERATURE: 0.04\n",
      "tioning. \n",
      "KIRA: No p => reficies and be some with Dax? \n",
      "O'BRIEN: No, I can't see some zerrous. \n",
      "KIRA: Oh, you think yourse aboats you. \n",
      "KIRA: How do you know now. \n",
      "BASHIR: That's intereded. \n",
      "ODO: Don't retlice it weaking the information. \n",
      "ODO: I'm sorry, I don't know what you're tellod have my way to 3DaX Lecam. And we can say here at along. \n",
      "KIRA: Of course recessed 0now I don't 0n Bardas And of. \n",
      "KEIKO: Allor he what you done they can't believe it mare any difficualic as there a quarteis to the sutter and the death t \n",
      "----\n",
      "TEMPERATURE: 0.05\n",
      "tioning. \n",
      "KIRA: No p => rexamentagl 8ar'Ha and Bajor. \n",
      "QUARK: There are there was fundee that is 2Dare. It's a dozbs xundratimetic Rigo is at the give sure it's a Lisk \n",
      "nA I's lid todej you, sir. \n",
      "! I tolder : was 'lerol. me are not them Masce is a ? Eneighare you forGaying. \n",
      "DAX: They don't think you're not to meanify theaOd) \n",
      "O'BRIEN: What's the station. \n",
      ": Was are you? \n",
      "verbakioms but No offlact. \n",
      "KIRA: I'm sorry, I don't know. \n",
      "QUARK: So doner to +leeve no waiting. \n",
      "O'BRIEN: You're not tourter it's a long from the  \n"
     ]
    }
   ],
   "source": [
    "gen_length = 500\n",
    "seed_index = 10000\n",
    "\n",
    "for temp in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "\n",
    "    gen = generate_string(seed_index, temp, gen_length, model)\n",
    "\n",
    "    print(4*\"-\")\n",
    "    print(\"TEMPERATURE: \" + str(temp))\n",
    "    print(gen[:-gen_length], end=\"\")\n",
    "    print(\" => \", end = \"\")\n",
    "    print(gen[-gen_length:], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-collector",
   "metadata": {
    "id": "vzsYhZro71_x"
   },
   "source": [
    "Let's make a few observations. \n",
    "\n",
    "1. First of all, it can take a surprisingly long time to make predictions using our model. This is because we have to call the `predict()` method *for each character*, in order to ensure that the model appropriately takes into account its recent predictions. This can take a pretty long time! \n",
    "2. Second, determining a good value for the temperature can take some experimentation. Note that low temperatures don't necessarily correspond to \"more realistic\" text -- they just correspond to highlighting common patterns in the text, possibly in excess. Higher temperatures also don't necessarily correspond to a \"creative\" algorithm in any normal sense of the word -- set the temperature too high, and you'll just get gibberish. \n",
    "\n",
    "## Specialization\n",
    "\n",
    "In this case, we were able to create a model for generating Star Trek scripts using an instance of Google Colab in roughly 15 minutes. This model is highly limited. Although it clearly has learned some relevant features of Star Trek scripts, there's no way that you'd mistake the output of the model for an actual script by a screenwriter. Considering how hard this was, imagine how much effort and computational resources are required to create more general language models! Indeed, as highlighted in a [recent and controversial paper](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf), training large language models in this day and age can require energy expenditure comparable to a trans-Atlantic flight! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B] *",
   "language": "python",
   "name": "conda-env-PIC16B-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
