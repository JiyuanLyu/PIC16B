{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:PIC16B] *",
      "language": "python",
      "name": "conda-env-PIC16B-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dynamic-david"
      },
      "source": [
        "# Text Generation with Recurrent Neural Networks\n",
        "\n",
        "In this set of lecture notes, we'll consider a new kind of machine learning task. Previously, we've focused on *classification* problems. In classification problems, the goal is to assign a given piece of data to one of several categories. Today, we'll instead consider a simple  *generation* problem. A *generative* model can be used to create \"realistic\" examples after it's been trained. Generative models are at the heart of machine learning topics like deepfakes, language modeling, and [style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer).  \n",
        "\n",
        "\n",
        "\n",
        "*Parts of these lecture notes were based on [this tutorial](https://keras.io/examples/generative/lstm_character_level_text_generation/). It is recommended to run the code contained in these notes in a Google Colab instance with GPU acceleration enabled.* "
      ],
      "id": "dynamic-david"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silver-invalid"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ],
      "id": "silver-invalid",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZreMMb84HNi",
        "outputId": "20119480-b3b0-48cd-c58c-40abfed6f4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# link to Google Drive to read in trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "lZreMMb84HNi",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "headed-divorce"
      },
      "source": [
        "\n",
        "## Our Task\n",
        "\n",
        "Today, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of *Star Trek: Deep Space Nine.*\n",
        "\n",
        "<figure class=\"image\" style=\"width:300px\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/DS9.jpg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "In particular, we are going to attempt to teach a neural  network to generate *episode scripts*. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance. \n"
      ],
      "id": "headed-divorce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intelligent-title"
      },
      "source": [
        "start_episode = 20 # Start in Season 2, Season 1 is not very good\n",
        "num_episodes = 50  # only pick this many episodes to train on\n",
        "\n",
        "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
        "star_trek_scripts = pd.read_json(url)\n",
        "\n",
        "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
        "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
        "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
        "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
        "    text = text.replace(char, \"\")"
      ],
      "id": "intelligent-title",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clinical-generator",
        "outputId": "2a045537-227c-4ecd-a345-0c6415d89125"
      },
      "source": [
        "print(text[0:500])"
      ],
      "id": "clinical-generator",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Last\n",
            "time on Deep Space Nine.  \n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call\n",
            "themselves the Circle. \n",
            "O'BRIEN: What gives them the right to mess up our station? \n",
            "ODO: They're an extremist faction who believe in Bajor for the\n",
            "Bajorans. \n",
            "SISKO: I can't loan you a Starfleet runabout without knowing where you\n",
            "plan on taking it. \n",
            "KIRA: To Cardassia Four to rescue a Bajoran prisoner of war. \n",
            "(The prisoners are rescued.) \n",
            "KIRA: Come on. We have a ship waiting. \n",
            "JARO: What you \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blank-smile"
      },
      "source": [
        "Our first step, as usual, is data preparation. What we need to do is format the data in such a way that we can treat the situation as a classification problem after all. That is: \n",
        "\n",
        "> Given a string of text, predict the next character in that string. \n",
        "\n",
        "Doing this repeatedly will allow the model to generate large bodies of text. \n",
        "\n",
        "To do this, we want to split our data like so: \n",
        "\n",
        "```\n",
        "predictor = \"to boldly g\"\n",
        "target    = \"o\"\n",
        "```\n",
        "\n",
        "The following function will do this for us. The `max_len` argument gives the number of characters that should be in the predictor string, and the `step_size` argument lets us skip indices if we want to in order to decrease the size of the data. "
      ],
      "id": "blank-smile"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "published-oklahoma"
      },
      "source": [
        "def split(raw_text, max_len, step_size = 1):\n",
        "\n",
        "    lines = []\n",
        "    next_chars = []\n",
        "\n",
        "    for i in range(0, len(text) - max_len, step_size):\n",
        "        lines.append(text[i:i+max_len])\n",
        "        next_chars.append(text[i+max_len])\n",
        "    \n",
        "    return lines, next_chars"
      ],
      "id": "published-oklahoma",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immediate-filling",
        "outputId": "5eac31d6-9f17-4c74-cd25-6f85cb4431dd"
      },
      "source": [
        "max_len = 20\n",
        "\n",
        "lines, next_chars =  split(text, max_len = max_len, step_size = 5)\n",
        "for i in range(10, 15):\n",
        "    print(lines[i] + \"     =>    \" + next_chars[i])"
      ],
      "id": "immediate-filling",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he emblem of the All     =>    i\n",
            "blem of the Alliance     =>     \n",
            "of the Alliance for      =>    G\n",
            "e Alliance for Globa     =>    l\n",
            "iance for Global Uni     =>    t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "processed-flesh"
      },
      "source": [
        "Our next step is to vectorize the characters. This is similar to the word vectorization task, but it's simple enough in this case that's arguably more convenient to actually handle it outside of TensorFlow. It is also possible to handle vectorization using TensorFlow tools, as demonstrated in [this tutorial](https://www.tensorflow.org/tutorials/text/text_generation). "
      ],
      "id": "processed-flesh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "radical-aberdeen"
      },
      "source": [
        "chars = sorted(set(text))\n",
        "char_indices = {char : chars.index(char) for char in chars}\n",
        "X = np.zeros((len(lines), max_len, len(chars)))\n",
        "y = np.zeros((len(lines), 1), dtype = np.int32)\n",
        "for i, line in enumerate(lines):\n",
        "\tfor t, char in enumerate(line):\n",
        "\t\tX[i, t, char_indices[char]] = 1\n",
        "\ty[i] = char_indices[next_chars[i]]"
      ],
      "id": "radical-aberdeen",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaded-white"
      },
      "source": [
        "Let's take a look at what happened here: "
      ],
      "id": "loaded-white"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "becoming-renewal",
        "outputId": "17688fb3-c491-4f25-e3f7-d896c49eff7d"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "id": "becoming-renewal",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((314163, 20, 78), (314163, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "divine-fancy",
        "outputId": "4f279cee-d2d0-4e50-b4eb-e66c3c00ff54"
      },
      "source": [
        "X[0]"
      ],
      "id": "divine-fancy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "express-ambassador",
        "outputId": "3ff51377-230e-4eee-c545-6d09125f9b41"
      },
      "source": [
        "y[0]"
      ],
      "id": "express-ambassador",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fourth-hampshire"
      },
      "source": [
        "Now we're ready to perform a train-test split: "
      ],
      "id": "fourth-hampshire"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "limiting-evans"
      },
      "source": [
        "train_len = int(0.7*X.shape[0])\n",
        "X_train = X[0:train_len]\n",
        "X_val = X[train_len:]\n",
        "\n",
        "y_train = y[0:train_len]\n",
        "y_val  = y[train_len:]"
      ],
      "id": "limiting-evans",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-treasurer"
      },
      "source": [
        "Model time! We'll use a simple *Long Short-Term Memory* (LSTM) model for this example. LSTMs are one example of *recurrent* neural network layers. Here's a diagram illustrating the schematic functioning of a recurrent layer. \n",
        "\n",
        "![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "*Image credit: [Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), OpenAI*\n",
        "\n",
        "On the lefthand side, we have a \"zoomed out\" picture of a recurrent neural network layer. On the righthand side, we see the \"zoomed in\" version. The key point here is that output $h_2$ depends not only on input $x_2$, but also, indirectly, on inputs $x_0$ and $x_1$. This means that recurrent neural networks are highly suitable for modeling processes that have temporal structure. Text is an example: the last few characters are the \"history\" of the text. Timeseries data are another clear example, and indeed, we can use a very similar workflow to the one we'll use today in order to do forecasting in timeseries. \n",
        "\n",
        "Since training for this kind of task gets expensive fast, we'll use just one LSTM layer followed by a `Dense` output layer. "
      ],
      "id": "fantastic-treasurer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alleged-stephen"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    layers.LSTM(128, name = \"LSTM\", input_shape=(max_len, len(chars))),\n",
        "    layers.Dense(len(chars), activation = \"softmax\")        \n",
        "])"
      ],
      "id": "alleged-stephen",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imported-syntax"
      },
      "source": [
        "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "              optimizer = \"adam\")"
      ],
      "id": "imported-syntax",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "third-primary"
      },
      "source": [
        "Time for training. We'll do just one epoch for now, mostly just to prove that we've set up our model correctly. "
      ],
      "id": "third-primary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sensitive-senior",
        "outputId": "9dfdd9bf-aabd-405a-b555-355c2b6fcff9"
      },
      "source": [
        "# code I used to train and save the model\n",
        "# model.fit(X_train, \n",
        "#           y_train,\n",
        "#           validation_data= (X_val, y_val),\n",
        "#           batch_size=128, epochs = 200)\n",
        "# model.save('/content/drive/MyDrive/DS9_model') \n",
        "\n",
        "model.fit(X_train, \n",
        "          y_train,\n",
        "          validation_data= (X_val, y_val),\n",
        "          batch_size=128, epochs = 1)"
      ],
      "id": "sensitive-senior",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1719/1719 [==============================] - 42s 6ms/step - loss: 2.9706 - val_loss: 2.1960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0570484f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blond-tradition"
      },
      "source": [
        "Rather than training the entire model live during lecture, I'm instead going to load in a saved model that I previously trained for 200 epochs on Google Colab. On Colab, each epoch takes around 10s or so. 200 epochs corresponds to roughly 30 minutes. "
      ],
      "id": "blond-tradition"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equal-seattle"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DS9_model')"
      ],
      "id": "equal-seattle",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acquired-trout"
      },
      "source": [
        "Generative models define *probability distributions* over the space of possible outputs. So, our overall algorithm is going to generate new text in a partially randomized way. To make this happen, we define a `sample` function which will take the model outputs, turn them into probabilities, and then sample from the probabilities to produce a single character (well, technically, an integer corresponding to a single character). \n",
        "\n",
        "An important parameter here is the so-called *temperature* (this terminology comes from statistical physics. When the temperature is high, the model will more frequently choose low-probability characters. This is sometimes interpreted as \"creativity,\" and leads to more unpredictable outputs. When the temperature is low, on the other hand, the model will \"play it safe\" and tend to stick to known patterns. In the extreme limiting case as the temperature approaches 0, the model will ultimately get stuck in \"loops\" in which it repeats common phrases over and over again. "
      ],
      "id": "acquired-trout"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joined-population"
      },
      "source": [
        "def sample(preds, temp):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    probs = np.exp(preds/temp)\n",
        "    probs = probs / probs.sum()\n",
        "    samp = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samp)"
      ],
      "id": "joined-population",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suited-tunnel"
      },
      "source": [
        "Now that we know how to sample from the model predictions to create a new character, let's now define a convenient function that will allow us to create entire strings of specified length using this process. "
      ],
      "id": "suited-tunnel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "floating-yacht"
      },
      "source": [
        "def generate_string(seed_index, temp, gen_length, model): \n",
        "\n",
        "    gen_seq = np.zeros((max_len + gen_length, len(chars)))\n",
        "    seed = X[seed_index]\n",
        "    gen_seq[0:max_len] = seed\n",
        "    \n",
        "    gen_text = lines[seed_index]\n",
        "\n",
        "    for i in range(0, gen_length):\n",
        "        window = gen_seq[i: i + max_len]\n",
        "        preds = model.predict(np.array([window]))[0]\n",
        "        next_index = sample(preds, temp)\n",
        "        gen_seq[max_len + i, next_index] = True\n",
        "\n",
        "        next_char = chars[next_index]\n",
        "        gen_text += next_char\n",
        "\n",
        "    return(gen_text)"
      ],
      "id": "floating-yacht",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chinese-trauma"
      },
      "source": [
        "Let's try it out! "
      ],
      "id": "chinese-trauma"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "presidential-depression",
        "outputId": "51c483d8-c6a8-4b79-910e-c91081a7ec5c"
      },
      "source": [
        "gen_length = 500\n",
        "seed_index = 10000\n",
        "\n",
        "for temp in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
        "\n",
        "    gen = generate_string(seed_index, temp, gen_length, model)\n",
        "\n",
        "    print(4*\"-\")\n",
        "    print(\"TEMPERATURE: \" + str(temp))\n",
        "    print(gen[:-gen_length], end=\"\")\n",
        "    print(\" => \", end = \"\")\n",
        "    print(gen[-gen_length:], \"\")"
      ],
      "id": "presidential-depression",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----\n",
            "TEMPERATURE: 0.01\n",
            "tioning. \n",
            "KIRA: No p => rourcted to know why it was the station? \n",
            "KIRA: I think the computer datages and you know her. \n",
            "BASHIR: Oh, nothing that I've had to set in the conduines. \n",
            "SISKO: There is one that you'd be the wormhole. I don't know what we're going to make me tries to realise. I don't know. \n",
            "SISKO: We've been have the station. \n",
            "ODO: It's transpuse and the one was to get the pating show you? \n",
            "KIRA: Commander, I decimet had looking for the system. \n",
            "KIRA: Oh, did hates, you're a ship. \n",
            "DAX: I'm not sure our senso \n",
            "----\n",
            "TEMPERATURE: 0.02\n",
            "tioning. \n",
            "KIRA: No p => roblem still a/radget it. \n",
            "O'BRIEN: Majon. \n",
            "ODO: I say the desien lyve you have to be a ship. \n",
            "ODO: She was wrong. \n",
            "KIRA: Loog make state want to have a little move on our engine\n",
            "never to get anything. \n",
            "ODO: Oh, don't you, I know he were to do about? \n",
            "KEIKO: I think the one who kind of anymoly. \n",
            "DAX: You must have been trying to have to do for up in resupences, but you know that. \n",
            "O'BRIEN: What was hat of the Federation of a big privintuation of the straighty believes. \n",
            "SISKO: I don't know that  \n",
            "----\n",
            "TEMPERATURE: 0.03\n",
            "tioning. \n",
            "KIRA: No p => roblem still and with the Dominion. \n",
            "KIRA: I don't like me to keep him. \n",
            "KIRA: You knew I was the engine on the station Kaik to the other few to the station of the very reporsed and uppersed the station traffit \n",
            "SISKO: Go are relay, I want to take you were surpreate securet. \n",
            "ODO: When the Federation and some personstine of : Alrine him of the shipsed five heading to hat you, Commander, I have a ready. If you prous . \n",
            "ODO: I think you're get your (when I middanterer's onee said the station to be \n",
            "----\n",
            "TEMPERATURE: 0.04\n",
            "tioning. \n",
            "KIRA: No p => roblem still and information to some mideray. As I'm and sire the shuth. \n",
            "SISKO: I7 seroudd it to take back. \n",
            "SISKO: I don't know that the transtons, we're becoust her against her shipt it start on the ship. \n",
            "DAX: I'm not sure our sensors areved. \n",
            "KIRA: They're not and the one whose nace like a lot of the Cardassian zones to be axpose. \n",
            "O'BRIEN: How clossed you are an Epporess a long has aga.) \n",
            "KIRA: Then we've been before to kelier the house is out of the replions erer\n",
            "for to come unicery back  \n",
            "----\n",
            "TEMPERATURE: 0.05\n",
            "tioning. \n",
            "KIRA: No p => roblem svirty fire how a reading and the Dominion stousing better. \n",
            "QUARK: Ones enother Ender.) \n",
            "SISKO: Oh, do5ars pressages. \n",
            "BASHIR: Commander. I think the computer Rozakat. \n",
            "DAX: lites it quadter. \n",
            "O'BRIEN: They're talking. \n",
            "NATIMA: I think you're not surondwate. \n",
            "O'BRIEN: They maybal I isomine that ? \n",
            "/EM: ARlious. \n",
            "(Shohoustar dabs before thatLa lirting to the damazia.) \n",
            "plere there in the station with the conduital subspiginan and  un on ! \n",
            "DERAL: Ha! \n",
            "KEIKO: You know I'd be recend the tru \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirty-outside"
      },
      "source": [
        "Let's make a few observations. \n",
        "\n",
        "1. First of all, it can take a surprisingly long time to make predictions using our model. This is because we have to call the `predict()` method *for each character*, in order to ensure that the model appropriately takes into account its recent predictions. This can take a pretty long time! \n",
        "2. Second, determining a good value for the temperature can take some experimentation. Note that low temperatures don't necessarily correspond to \"more realistic\" text -- they just correspond to highlighting common patterns in the text, possibly in excess. Higher temperatures also don't necessarily correspond to a \"creative\" algorithm in any normal sense of the word -- set the temperature too high, and you'll just get gibberish. \n",
        "\n",
        "## Specialization\n",
        "\n",
        "In this case, we were able to create a model for generating Star Trek scripts using an instance of Google Colab in roughly 30 minutes. This model is highly limited. Although it clearly has learned some relevant features of Star Trek scripts, there's no way that you'd mistake the output of the model for an actual script by a screenwriter. Considering how hard this was, imagine how much effort and computational resources are required to create more general language models! Indeed, as highlighted in a [recent and controversial paper](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf), training large language models in this day and age can require energy expenditure comparable to a trans-Atlantic flight! "
      ],
      "id": "dirty-outside"
    }
  ]
}