{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "included-edition",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks and Image Classification\n",
    "\n",
    "The image classification problem is the problem of assigning a label to an image. For example, we might want to assign the label \"duck\" to pictures of ducks, the label \"frog\" to pictures of frogs, and so on. \n",
    "\n",
    "In this lecture, we'll introduce some of the most important tools for image classification: convolutional neural networks. Major parts of this lecture are based on the \"Images\" tutorial [here](https://www.tensorflow.org/tutorials/images/cnn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-thing",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "For this lecture, we'll use a subset of the [CIFAR-10 data set](https://www.cs.toronto.edu/~kriz/cifar.html). This data set can be conveniently accessed using a method from `tensorflow.keras.datasets`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-wholesale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sunset-listening",
   "metadata": {},
   "source": [
    "There are 50,000 training images and 10,000 test images. Each image has 32x32 pixels, and there are three color \"channels\" -- red, green, and blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-locking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "endangered-transparency",
   "metadata": {},
   "source": [
    "There are 10 classes of image, encoded by the `labels` arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-ordering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "marine-reminder",
   "metadata": {},
   "source": [
    "Each class corresponds to a type of object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-taxation",
   "metadata": {},
   "source": [
    "Let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-somalia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "straight-worker",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Convolution is a mathematical operation commonly used to extract *features* (meaningful properties) from images. The idea of image convolution is pretty simple. We define a *kernel* matrix containing some numbers, and we \"slide it over\" the input data. At each location, we multiply the data values by the kernel matrix values, and add them together. Here's an illustrative diagram: \n",
    "\n",
    "![](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "*Image from [Dive Into Deep Learning](https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)*\n",
    "\n",
    "The value of 19 in the output is obtained in this example by computing $0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19$. \n",
    "\n",
    "This operation might seem either abstract or trivial, but it can be used to extract useful image features. For example, let's manually define a kernel and use it to perform \"edge detection\" in a greyscale image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-weekly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-chapter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-flexibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rubber-burst",
   "metadata": {},
   "source": [
    "Observe that the convolved image (right) has darker patches corresponding to the distinct \"edges\" in the image, where darker colors meet lighter colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-bones",
   "metadata": {},
   "source": [
    "## Learning Kernels\n",
    "\n",
    "There are lots of convolutional kernels we could potentially use. How do we know which ones are meaningful? In practice, we don't. So, we treat them as parameters, and learn them from data as part of the model fitting process. This is exactly what the `Conv2d` layer allows us to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-donna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-rubber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "future-hollow",
   "metadata": {},
   "source": [
    "Let's compare a few other possibilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-bedroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "moderate-diploma",
   "metadata": {},
   "source": [
    "These features may or may not be informative -- they are purely random! We can try to learn informative features by embedding these kernels in a model and optimizing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-boost",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "\n",
    "The most common approach is to alternate `Conv2D` layers with `MaxPooling2D` layers. Pooling layers act as \"summaries\" that reduce the size of the data at each step. After we're done doing \"2D stuff\" to the data, we then need to `Flatten` the data from 2d to 1d in order to pass it through the final `Dense` layers, which form the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-finance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "separate-allen",
   "metadata": {},
   "source": [
    "What does max pooling do? you can think of it as a kind of \"summarization\" step in which we intentionally make the current output somewhat \"blockier.\" Technically, it involves sliding a window over the current batch of data and picking only the largest element within that window. Here's an example of how this looks: \n",
    "\n",
    "![](https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png)\n",
    "\n",
    "*Image credit: Computer Science Wiki*\n",
    "\n",
    "Ok, now that we know what each of our layers are doing, we can now inspect our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-creation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interracial-lighter",
   "metadata": {},
   "source": [
    "Let's train our model and see how it does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-restoration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alpine-appointment",
   "metadata": {},
   "source": [
    "After just a few rounds of training, our model is able to guess the image label more than 50% of the time on the unseen validation data, which is relatively impressive considering that there are 10 possibilities. \n",
    "\n",
    "Note: the training process can often be considerably accelerated by training on a GPU. A limited amount of free GPU power is available via Google Colab, and is illustrated [here](https://colab.research.google.com/notebooks/gpu.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-expert",
   "metadata": {},
   "source": [
    "## Extracting Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-consultation",
   "metadata": {},
   "source": [
    "Let's see how our model did on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-spencer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-flesh",
   "metadata": {},
   "source": [
    "We'll plot these predicted labels along side the (true labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "built-philadelphia",
   "metadata": {},
   "source": [
    "Overall, these results look fairly reasonable. There are plenty of mistakes, but it does look like the places where the model made errors are authentically somewhat confusing. A more complex or powerful model would potentially be able to do noticeably better on this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-assessment",
   "metadata": {},
   "source": [
    "# Visualizing Learned Features \n",
    "\n",
    "It's possible to define a separate model that allows us to study the features learned by the model. These are often called *activations*. We create this model by simply asserting that the model outputs are equal to the outputs of the first convolutional layer. For this we use the `models.Model` class rather than the `models.Sequential` class, which is more convenient but less flexible. \n",
    "\n",
    "It's possible to look at the activations at different levels of the model. Generally speaking, it is expected that the activations become more abstract as one goes higher up the model structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-conservative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-actress",
   "metadata": {},
   "source": [
    "Now we can compute the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-maryland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "improved-promise",
   "metadata": {},
   "source": [
    "And visualize them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-kuwait",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "effective-sudan",
   "metadata": {},
   "source": [
    "Somewhat romantically, these activations might be interpreted as \"how the algorithm looks at\" the resulting image. That said, one must be careful of over-interpretation. Still, it looks like some of the features correspond to edge detection (like we saw above), while others correspond to highlighting different patches of colors, enabling, for example, separation of the foreground object from the background. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
