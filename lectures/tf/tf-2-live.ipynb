{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retained-supplement",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks and Image Classification\n",
    "\n",
    "The image classification problem is the problem of assigning a label to an image. For example, we might want to assign the label \"duck\" to pictures of ducks, the label \"frog\" to pictures of frogs, and so on. \n",
    "\n",
    "In this lecture, we'll introduce some of the most important tools for image classification: convolutional neural networks. Major parts of this lecture are based on the \"Images\" tutorial [here](https://www.tensorflow.org/tutorials/images/cnn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-wesley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fresh-challenge",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "For this lecture, we'll use a subset of the [CIFAR-10 data set](https://www.cs.toronto.edu/~kriz/cifar.html). This data set can be conveniently accessed using a method from `tensorflow.keras.datasets`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-warning",
   "metadata": {},
   "source": [
    "There are 50,000 training images and 10,000 test images. Each image has 32x32 pixels, and there are three color \"channels\" -- red, green, and blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-relay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "polished-fault",
   "metadata": {},
   "source": [
    "There are 10 classes of image, encoded by the `labels` arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-orchestra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "technical-customs",
   "metadata": {},
   "source": [
    "Each class corresponds to a type of object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-corruption",
   "metadata": {},
   "source": [
    "Let's take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "electrical-ozone",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Convolution is a mathematical operation commonly used to extract *features* (meaningful properties) from images. The idea of image convolution is pretty simple. We define a *kernel* matrix containing some numbers, and we \"slide it over\" the input data. At each location, we multiply the data values by the kernel matrix values, and add them together. Here's an illustrative diagram: \n",
    "\n",
    "![](https://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "*Image from [Dive Into Deep Learning](https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html)*\n",
    "\n",
    "The value of 19 in the output is obtained in this example by computing $0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19$. \n",
    "\n",
    "This operation might seem either abstract or trivial, but it can be used to extract useful image features. For example, let's manually define a kernel and use it to perform \"edge detection\" in a greyscale image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-shanghai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grateful-responsibility",
   "metadata": {},
   "source": [
    "Observe that the convolved image (right) has darker patches corresponding to the distinct \"edges\" in the image, where darker colors meet lighter colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-channel",
   "metadata": {},
   "source": [
    "## Learning Kernels\n",
    "\n",
    "There are lots of convolutional kernels we could potentially use. How do we know which ones are meaningful? In practice, we don't. So, we treat them as parameters, and learn them from data as part of the model fitting process. This is exactly what the `Conv2d` layer allows us to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-iceland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick an individual image while preserving dimensions\n",
    "\n",
    "\n",
    "# perform convolution and extract as numpy array\n",
    "\n",
    "\n",
    "# get a single feature (corresponding to one choice of convolution)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-answer",
   "metadata": {},
   "source": [
    "Let's compare a few other possibilities: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(3, 3, figsize = (8, 6))\n",
    "\n",
    "axarr[0,0].imshow(color_im[0])\n",
    "axarr[0,0].axis(\"off\")\n",
    "axarr[0,0].set(title = \"Original\")\n",
    "\n",
    "i = 0\n",
    "for ax in axarr.flatten()[1:]:\n",
    "    ax.imshow(convd[0,:,:,i], cmap = \"gray\")\n",
    "    i += 1\n",
    "    ax.axis(\"off\")\n",
    "    ax.set(title = \"Feature \" + str(i))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-charge",
   "metadata": {},
   "source": [
    "These features may or may not be informative -- they are purely random! We can try to learn informative features by embedding these kernels in a model and optimizing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-launch",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "\n",
    "The most common approach is to alternate `Conv2D` layers with `MaxPooling2D` layers. Pooling layers act as \"summaries\" that reduce the size of the data at each step. After we're done doing \"2D stuff\" to the data, we then need to `Flatten` the data from 2d to 1d in order to pass it through the final `Dense` layers, which form the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-borough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-dynamics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "material-vatican",
   "metadata": {},
   "source": [
    "Let's train our model and see how it does! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-variance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-strain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tender-receptor",
   "metadata": {},
   "source": [
    "After just a few rounds of training, our model is able to guess the image label more than 50% of the time on the unseen validation data, which is relatively impressive considering that there are 10 possibilities. \n",
    "\n",
    "Note: the training process can often be considerably accelerated by training on a GPU. A limited amount of free GPU power is available via Google Colab, and is illustrated [here](https://colab.research.google.com/notebooks/gpu.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-recommendation",
   "metadata": {},
   "source": [
    "# Visualizing Learned Features \n",
    "\n",
    "It's possible to define a separate model that allows us to study the features learned by the model. These are often called *activations*. We create this model by simply asserting that the model outputs are equal to the outputs of the first convolutional layer. For this we use the `models.Model` class rather than the `models.Sequential` class, which is more convenient but less flexible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neither-marketplace",
   "metadata": {},
   "source": [
    "Now we can compute the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-somerset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "contained-supervisor",
   "metadata": {},
   "source": [
    "And visualize them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "criminal-conditioning",
   "metadata": {},
   "source": [
    "While one has to be careful about over-interpreting these activations, it looks like some of the features correspond to edge detection (like we saw above), while others correspond to highlighting different patches of colors, enabling, for example, separation of the horse's body from the background. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B] *",
   "language": "python",
   "name": "conda-env-PIC16B-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
