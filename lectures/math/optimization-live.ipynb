{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The One-Lecture Story of Optimization for Machine Learning\n",
    "\n",
    "**Warning: these lecture notes contain an animation with rapidly moving lines and colors.**\n",
    "\n",
    "In this lecture, we'll go over a simple example that illustrates some of the important ideas of *optimization for machine learning.* \n",
    "\n",
    "*Optimization* is the mathematical study of techniques for finding points at which functions reach their maximum or minimum values. For example, the minimum value of the function $f(x) = x^2$ is the point $x = 0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization techniques are used to obtain similar insights for more complicated functions. \n",
    "\n",
    "The reason that we care about optimization in the context of machine learning is that the \"training\" step of most machine learning methods is nothing other than optimizing a function. As you may remember, the prototypical (supervised) machine learning task has the following form:\n",
    "\n",
    "> Find a model $f$ from a collection $\\mathcal{M}$ of possible models such that $\\mathcal{L}(f, X, y)$ is minimized.  \n",
    "\n",
    "In this problem statement: \n",
    "\n",
    "- The *model* $f$ is some kind of function that takes in *predictor data* $X$ and spits out a prediction $\\hat{y}$ of the *target* data $y$. \n",
    "- $\\mathcal{M}$ is a set of all possible models under consideration. Usually this set is parameterized, so that one chooses a model from $\\mathcal{M}$ by choosing the values of one or more parameters. \n",
    "- $\\mathcal{L}$ is a *loss function* that is adapted to our problem scenario. Roughly, the loss function should be designed so that it is small when the model \"fits the data\" well. What it means to \"fit the data\" depends on the problem! \n",
    "\n",
    "If you're feeling a little unsure about terms like \"model\" or \"predictor data\" or \"loss function,\" you might find it helpful to consult [this lecture on basic concepts](https://youtu.be/TOjJcMR053I) ([notes](https://philchodrow.github.io/PIC16A/content/ML/ML_1.jpg)) or [this discussion](https://youtu.be/l-UkQoBlgek) of doing linear regression by hand ([notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/ML/ML_2.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to illustrate the ideas in this lecture with our good friend, linear regression. In our standard linear regression setup, we have some predictor data $X$ and some target data $y$, and we aim to approximate: \n",
    "\n",
    "$$ y_i \\approx \\hat{y}_i = \\beta_0 + \\sum_j \\beta_j x_{ij},$$\n",
    "\n",
    "where $y_i$ is the $i$th value of the target data and $x_{ij}$ is the element in the $i$th row and $j$th column of the predictor data. \n",
    "\n",
    "In this setup, a choice of model corresponds to a choice of coefficients $\\beta$.  \n",
    "\n",
    "This model is appropriate when we would like to detect a linear trend in our data, like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of drawing a line through these data corresponds to the problem of  minimizing a chosen loss function. We usually use this function: \n",
    "\n",
    "$$L(\\beta, X, y) = \\sum_i \\left(\\beta_0 + \\sum_j \\beta_j x_{ij} - y_i\\right)^2 = \\sum_i (y_i - \\hat{y}_i)^2\\;.$$\n",
    "\n",
    "This is the \"sum of square residuals\" loss, and for those of you comfortable with matrix-vector and norm notations, it can also be written \n",
    "\n",
    "$$L(\\beta, X, y) = \\lVert X\\beta - y \\rVert^2\\;, $$\n",
    "\n",
    "where we regard $\\beta$ as a vector of all the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a recent lecture, we saw that it's possible to perform linear regression using the `np.lstsq()` function. This function allows us to form estimates of $\\beta$ by minimizing the least-squares loss. Let's do a quick review for our sample data. \n",
    "\n",
    "In this case, we have two coefficients: a slope and an intercept, which I called `b0` and `b1` above. We can do least-squares estimation to get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # true values were b0 = -0.2 and b1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: the use of `np.c_` allowed me to add a column of ones to my predictor data `X`. For math reasons, this allows us to estimate $\\beta_0$, which wouldn't otherwise be possible. \n",
    "\n",
    "Let's visualize our solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, that looks like a pretty reasonable linear trend for these data. \n",
    "\n",
    "## How did we get here? \n",
    "\n",
    "Here's an *algorithmic* question: what does `np.lstsq()` actually do? Or, more generally: \n",
    "\n",
    "> How does an optimization algorithm actually *find* a good value for the parameters? \n",
    "\n",
    "This is a very big question, and people spend their entire careers studying specific optimization algorithms. In the remainder of this lecture, I'd like to discuss the idea of one specific optimization algorithm, which has become especially important for its applications in machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "*Stochastic gradient descent* (SGD) is a very fancy phrase for a simple very idea. Next time you would like to impress an attractive person at a party, simply say to them: \"let us discuss stochastic gradient descent.\" They will be very impressed indeed. \n",
    "\n",
    "Before we talk about any more math, let's look at a picture of what's going on. I'm going to run a lot of code now! We're not going to talk about all the code in this lecture, but we are going to talk about a few of the pieces. \n",
    "\n",
    "Sidebar: if you are interested in making animations in Matplotlib, the below is a decent example to get you started. I based the parts of the code that manage the animation on [this example](https://matplotlib.org/stable/gallery/animation/bayes_update.html) in the Matplotlib docs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatePlot:\n",
    "    \"\"\"\n",
    "    an update class for a linear regression stochastic gradient descent (SGD)\n",
    "    animation. Takes instance variables corresponding to plot objects, \n",
    "    parameters and estimates, and animation controls. Calling the class\n",
    "    updates the plot. \n",
    "    \n",
    "    ax: array of 2 axes on which to plot\n",
    "    b0, b1: floats, true parameters of linear regression\n",
    "    b0_, b1_: floats, initial estimates of the true parameters\n",
    "    eps: float, learning rate\n",
    "    n: int, number of data points\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ax, b0, b1, b0_, b1_, eps = 0.01, n = 100):\n",
    "        \n",
    "        # store the axes on which to plot\n",
    "        self.ax0 = ax[0]\n",
    "        self.ax1 = ax[1]\n",
    "        \n",
    "        # true coefficients\n",
    "        self.b0 = b0\n",
    "        self.b1 = b1\n",
    "        \n",
    "        # initial coefficient estimates\n",
    "        self.b0_ = b0_\n",
    "        self.b1_ = b1_\n",
    "        \n",
    "        # number of data points\n",
    "        self.n = n\n",
    "        \n",
    "        # create data\n",
    "        self.X = np.random.rand(n)\n",
    "        self.y = self.b0 + self.b1*self.X + 0.1*np.random.randn(n)\n",
    "        self.X = self.X.reshape(n,1)\n",
    "        \n",
    "        # used for plotting lines\n",
    "        self.x_space = np.linspace(0, 1, 10)\n",
    "        \n",
    "        # show data on lefthand plot\n",
    "        self.ax0.scatter(self.X, self.y, color = \"grey\", s = 4, zorder = 100)\n",
    "        \n",
    "        # prepare animation variables\n",
    "        self.point = self.ax0.scatter([], [], color = \"red\", zorder = 200)\n",
    "        self.line, = self.ax0.plot([], [], 'k-')\n",
    "        self.loss, = self.ax1.plot([], [], 'k-')\n",
    "        \n",
    "        # learning rate\n",
    "        self.eps = eps\n",
    "        \n",
    "        # timesteps and value of loss function\n",
    "        self.t = []\n",
    "        self.L = []\n",
    "        \n",
    "    def __call__(self, i):\n",
    "        \"\"\"\n",
    "        makes the class callable, resulting in plot updates\n",
    "        each call performs a single step of SGD and returns\n",
    "        appropriate artists. i represents the algorithm \n",
    "        timestep/animation frame. \n",
    "        \"\"\"\n",
    "        \n",
    "        # pick a random point\n",
    "        j = np.random.randint(self.n)      \n",
    "        x, y = self.X[j,0], self.y[j]\n",
    "        \n",
    "        # update estimates with gradient of loss function evaluated \n",
    "        # at that point with respect to the coefficients\n",
    "        # \"this is the math\"\n",
    "        self.b0_ -= self.eps*2*(self.b1_ * x + self.b0_ - y)\n",
    "        self.b1_ -= self.eps*2*(self.b1_ * x + self.b0_ - y)*x\n",
    "        \n",
    "        # evaluate loss function \n",
    "        # \"this is the other math\"\n",
    "        L = sum((self.b1_*self.X[:,0] + self.b0_ - self.y)**2)\n",
    "        \n",
    "        # update lefthand plot with highlight point and new regression\n",
    "        # line\n",
    "        self.point.set_offsets([[x, y]])\n",
    "        self.line.set_data(self.x_space, self.b1_*self.x_space +self.b0_)\n",
    "        \n",
    "        # update timestep and loss function\n",
    "        self.t.append(i)\n",
    "        self.L.append(L)\n",
    "        \n",
    "        # plot loss function\n",
    "        self.loss.set_data(self.t, self.L)\n",
    "        \n",
    "        # return artists\n",
    "        return [self.point, self.line, self.loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of algorithm iterations\n",
    "n_steps = 200\n",
    "\n",
    "# create plotting background\n",
    "fig, ax = plt.subplots(1, 2, figsize = (7, 3))\n",
    "\n",
    "ax[0].set_xlim(0, 1)\n",
    "ax[0].set_ylim(-0.5, 0.5)\n",
    "\n",
    "ax[1].set_xlim(0, n_steps)\n",
    "ax[1].set_ylim(0, 5)\n",
    "ax[1].grid(True)\n",
    "\n",
    "ax[0].set(title = \"Regression Problem\", \n",
    "          xlabel = r\"$x$\",\n",
    "          ylabel = r\"$y$\")\n",
    "\n",
    "ax[1].set(title = \"Current Loss\",\n",
    "          xlabel = \"Iteration\",\n",
    "          ylabel = r\"$\\mathcal{L}$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# create the plot updater\n",
    "ud = UpdatePlot(ax, -0.2, 0.5, 0, 0, eps = 0.1, n = 100)\n",
    "anim = FuncAnimation(fig, ud, frames=n_steps, interval=200, blit=True)\n",
    "\n",
    "# to save as gif\n",
    "# anim.save('regression_sgd.gif',  fps=20, dpi = 200) \n",
    "\n",
    "# to interactively display in notebook\n",
    "plt.close() # prevents plot from showing twice\n",
    "HTML(anim.to_jshtml()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? \n",
    "\n",
    "On the **lefthand side**, we can see a sequence of possible linear models, which change over time. In each step of the algorithm:\n",
    "\n",
    "1. We select a random data point (red highlight). \n",
    "2. We ask that point what its opinion is about the best value of the parameters. \n",
    "3. We then push the parameters a little bit in the direction of that point's opinion. \n",
    "\n",
    "On the righthand side, we can see the loss function from above, that measures the quality of the fit. Smaller values are better, and we can see that the loss function does indeed tend to decrease over time. That said, we can also see little spikes where the loss function actually increases. This is due to \"weird\" points, who have opinions about the coefficients that deviate significantly from the average. Often, these are statistical outliers.  \n",
    "\n",
    "A bit more mathematically, at each timestep, we pick a random point $j$ and then do this: \n",
    "\n",
    "$$ \\hat{\\beta}_t = \\hat{\\beta}_{t-1} - \\epsilon \\nabla_{\\hat{\\beta}} \\ell(\\hat{\\beta}, x_j, y_j)$$\n",
    "\n",
    "If you're familiar with multivariable calculus, the term $\\nabla_{\\hat{\\beta}} \\ell(\\hat{\\beta}, x_j, y_j)$ is the gradient with respect to $\\hat{\\beta}$ of the single-point loss function \n",
    "\n",
    "$$\\ell(\\hat{\\beta}, x_j, y_j) = \\left(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_j - y_j\\right)^2\\;.$$\n",
    "\n",
    "In this case, it's possible to calculate the gradient directly: \n",
    "\n",
    "$$\n",
    "\\nabla_\\hat{\\beta} \\ell(\\hat{\\beta}, x_j, y_j) = \n",
    "2 (\\hat{\\beta}_0 + \\hat{\\beta}_1x_j - y_j)\n",
    "\\left(\\begin{matrix}\n",
    "1 \\\\ x_j\n",
    "\\end{matrix}\\right)\n",
    "$$\n",
    "\n",
    "The first component corresponds to $\\hat{\\beta}_0$, and the second component corresponds to $\\hat{\\beta}_1$. \n",
    "\n",
    "If you're not familiar with multivariable calculus, it's ok to think of $\\nabla_{\\hat{\\beta}} \\ell(\\hat{\\beta}, x_j, y_j)$ as expressing point $j$'s \"personal suggestion\" about how to improve the current values of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$. \n",
    "\n",
    "Now we're ready to unpack the phrase \"stochastic gradient descent.\" \n",
    "\n",
    "- **Stochastic**: at each stage, we pick a random (stochastic) data point. \n",
    "- **Gradient**: we compute that point's \"opinion\" about the parameters by using the gradient. \n",
    "- **Descent**: the loss function usually goes down (\"descends\") over time. \n",
    "\n",
    "Stochastic gradient descent is commonly used in many machine learning models, and is extremely popular in deep learning specifically. When we \"train\" or \"fit\" TensorFlow models, we'll often be using a relative of SGD under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing SGD for Linear Regression\n",
    "\n",
    "Let's now implement the SGD algorithm for linear algebra. The code in this section is the same code used to produce the animation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this lecture! In future lectures, we'll take our understanding of linear algebra and optimization into an extended study of practical machine learning via neural networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
