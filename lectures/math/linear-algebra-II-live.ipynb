{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra II\n",
    "\n",
    "In this set of notes, we'll consider singular value and eigenvalue problems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd                   # for pagerank example\n",
    "from matplotlib.image import imread   # for image compression\n",
    "from matplotlib import pyplot as plt  # for image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "There are lots of circumstances in which we should compute the singular value decomposition (SVD) or eigenvalue decomposition of a matrix. SVD is somewhat more general. \n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Everybody knows that the <a href=\"https://twitter.com/hashtag/SVD?src=hash&amp;ref_src=twsrc%5Etfw\">#SVD</a> is the <a href=\"https://twitter.com/hashtag/bestmatrixdecomposition?src=hash&amp;ref_src=twsrc%5Etfw\">#bestmatrixdecomposition</a> !!!<br><br> <a href=\"https://twitter.com/hashtag/UDVt?src=hash&amp;ref_src=twsrc%5Etfw\">#UDVt</a> <a href=\"https://twitter.com/hashtag/minimumreconstructionerror?src=hash&amp;ref_src=twsrc%5Etfw\">#minimumreconstructionerror</a> <a href=\"https://twitter.com/hashtag/maximalvariance?src=hash&amp;ref_src=twsrc%5Etfw\">#maximalvariance</a> <a href=\"https://twitter.com/hashtag/nonconvexbutstillsolvable?src=hash&amp;ref_src=twsrc%5Etfw\">#nonconvexbutstillsolvable</a> <a href=\"https://twitter.com/hashtag/nothanksQR?src=hash&amp;ref_src=twsrc%5Etfw\">#nothanksQR</a><a href=\"https://twitter.com/hashtag/seeyoulaterEigenDecomp?src=hash&amp;ref_src=twsrc%5Etfw\">#seeyoulaterEigenDecomp</a><a href=\"https://twitter.com/hashtag/blessed?src=hash&amp;ref_src=twsrc%5Etfw\">#blessed</a> <a href=\"https://t.co/je7Xetgfhr\">https://t.co/je7Xetgfhr</a></p>&mdash; Dr. Daniela Witten (@daniela_witten) <a href=\"https://twitter.com/daniela_witten/status/1279915517654888448?ref_src=twsrc%5Etfw\">July 5, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "As you may remember, a *[singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)* of a real matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is \n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\;, $$\n",
    "\n",
    "where $\\mathbf{D} \\in \\mathbb{R}^{m \\times n}$ has nonzero entries (the *singular values* $\\sigma_i$) only along its diagonal, and where $\\mathbf{U} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices. The singular values $\\sigma_i$ collectively give some measure of how \"large\" the matrix $\\mathbf{A}$ is. \n",
    "\n",
    "Numpy makes it easy to compute the SVD of a matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(1, 5, size = (7, 5))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the components of the SVD to reconstruct the original matrix $\\mathbf{A}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # == A up to numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low-rank approximation of A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # close to A, using only two singular vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Image Compression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the power of SVD, let's use it to *compress images*. In general, storing an image with $m \\times n$ greyscale pixels requires storing $m \\times n$ numbers, which can be quite expensive when $m$ or $n$ are large! SVD can be used to compress images by treating them as matrices and discarding the small singular values. Let's see an example! \n",
    "\n",
    "We will use an image of Maru. Maru is a Cat On The Internet who is famous for doing stuff like this: \n",
    "\n",
    "![](https://media2.giphy.com/media/9NAXEP3RiJjm8/giphy.webp?cid=ecf05e47dbjp1upvg5ovlw54yn1j9zrhaa73vg4x5m4er4ow&rid=giphy.webp&ct=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maru = imread(\"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\")\n",
    "plt.imshow(maru)\n",
    "plt.gca().axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic SVD for images only works on greyscale images -- there are ways to use SVD for color images, but this is beyond scope for today. The following simple function will convert the base image to greyscale: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_greyscale(im):\n",
    "    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function that encapsulates the steps we did in the previous section to compress the matrix, keeping only the top $k$ singular values in order to reconstruct the target matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a relatively clear picture with just 7% of the storage requirement! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue-Eigenvector Decomposition\n",
    "\n",
    "As you may remember, the eigenvalue-eigenvector decomposition for a matrix is related to the SVD. Eigenvalue-eigenvector decompositions are especially relevant in the case of symmetric matrices. \n",
    "\n",
    "> **Spectral Theorem for Symmetric Matrices**: if $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is a real symmetric matrix, then there exists a real diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n} $ and orthogonal matrix $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ such that $\\mathbf{A} = \\mathbf{U} \\Lambda\\mathbf{U}^T$. \n",
    "\n",
    "This theorem ensures that a real symmetric matrix $\\mathbf{A}$ possesses a full complement of *eigenvalue-eigenvector pairs* $(\\lambda, \\mathbf{u})$ which satisfy the equation \n",
    "\n",
    "$$\\mathbf{A}\\mathbf{u} = \\lambda \\mathbf{u}\\;.$$\n",
    "\n",
    "Numpy gives us a convenient function with which to calculate the eigenvalues and eigenvectors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first array gives the eigenvalues of $\\mathbf{A}$, while the second array gives the eigenvectors (as columns). We can check the eigenvalue-eigenvector condition above:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, it's useful to get the eigenvectors corresponding to the largest or smallest eigenvalues of a matrix. This can be done via sorting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvector with second-smallest eigenvalue\n",
    "# spoiler, you're going to need this one soon!\n",
    "\n",
    "\n",
    "# 2nd smallest eigenvalue and corresponding eigenvector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: PageRank\n",
    "\n",
    "The *PageRank score* provides a way to estimate the importance of entities in a network. Historically, PageRank was the algorithm used by Google to rank webpages: pages with higher PageRanks would appear higher in search results. \n",
    "\n",
    "Let's continue with the webpage example for a bit. PageRank works by allowing a \"random surfer\" to move around webpages by following links. Each time the surfer lands on a page, it then looks for all the links on that page. It then picks one at random and follows it, thereby arriving at the next page, where the process repeats. Because the surfer moves between linked pages, PageRank expresses an intuitive idea: **important pages are linked to other important pages.** [This diagram](https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.jpg) from Wikipedia gives a nice illustration. Note that more important webpages (higher PageRank) tend to be connected to other important webpages. \n",
    "\n",
    "<figure class=\"image\" style=\"width:50%\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8b/PageRanks-Example.jpg/1920px-PageRanks-Example.jpg\n",
    "\" alt=\"A set of 11 circles, with arrows between them. Some of the circles are larger than others, reflecting their high PageRank score. Large circles tend to be linked to other large circles by arrows.\" width=\"300px\">\n",
    "  <figcaption><i>A schematic for PageRank. </i></figcaption>\n",
    "</figure>\n",
    "\n",
    "PageRank can be estimated via random walks, and you may even have done this in PIC16A. However, the most efficient way to compute PageRank for small-to-medium-sized data is by computing eigenvectors of matrices. (The spectral theorem above does not apply because the relevant matrix is not symmetric, but this turns out not to be a problem). Let's take a look. \n",
    "\n",
    "Our data for this example comes from the hit Broadway musical \"Hamilton.\"\n",
    "\n",
    "<figure class=\"image\" style=\"width:20%\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/8/83/Hamilton-poster.jpg\" alt=\"The logo of the musical Hamilton, showing a silhouette dressed in period custom standing on top of a five-pointed star.\" width=\"300px\">\n",
    "  <figcaption><i>The Hamilton data set</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "The good folks at [The Hamilton Project](http://hamilton.newtfire.org/) analyzed the script for us, obtaining data on **who talks about whom** in each of the show's songs. When character A mentions character B, we'll think of this as a *link* from A to B, suggesting that B might be important. First we'll grab data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://philchodrow.github.io/PIC16A/homework/HW3-hamilton-data.csv\"\n",
    "df = pd.read_csv(url, names = [\"source\", \"target\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct the matrix of directed links: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct matrix of directed links \n",
    "# A[i,j] is the number of times that i mentions j\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we construct the so-called PageRank matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form the PageRank matrix\n",
    "\n",
    "# normalized adjacency matrix, corresponding to the transition\n",
    "# of a random walk\n",
    "\n",
    "# teleportation parameter\n",
    "\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leading eigenvector of the PageRank matrix is guaranteed to be real and positive with eigenvalue 1.0 (from the theory of Markov chains). We single this one out -- it's the one we want! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a nice display: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as a sorted data frame\n",
    "ranks = pd.DataFrame({\n",
    "    \"character\" : [N[i] for i in range(len(N))],\n",
    "    \"PageRank\"  : [s[i][0] for i in range(len(N))]\n",
    "})\n",
    "ranks.sort_values(\"PageRank\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Power of Linear Algebra, we have discovered that Alexander Hamilton is the main character of the musical *Hamilton*. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
