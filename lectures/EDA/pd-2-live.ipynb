{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600f9041",
   "metadata": {},
   "source": [
    "# Advanced Data Manipulation II: \"I Love Group-By\"\n",
    "\n",
    "In this lecture, we'll work on some additional skills for manipulating and analyzing tabular data. Our focus will be on: \n",
    "\n",
    "- **Filtering** data, identifying specific rows according to complex criteria. \n",
    "- **Aggregating** data, computing complicated summaries of groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df54b44",
   "metadata": {},
   "source": [
    "Let's start by reading in some data. We'll use the SQLite database that we created in a [recent lecture](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/sql/sql-1.ipynb) for this purpose. You may need to run the code in that lecture in order for the following block to correctly read in the data. Make sure that the string supplied to `sqlite3.connect()` points to the location of the database. \n",
    "\n",
    "We are going to extract measurements for stations south of -80 degrees latitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be248b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a7b652f",
   "metadata": {},
   "source": [
    "### Takeaways for Today\n",
    "\n",
    "1. The `transform()` and `apply()` methods can enable advanced computation on Pandas data frames using quite simple code. \n",
    "2. Pandas also offers specialized functions for common tasks, but getting the hang of `transform()` and `apply()` will take you far. \n",
    "3. Global warming is scary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f35af3",
   "metadata": {},
   "source": [
    "## Grouped Summaries\n",
    "\n",
    "We already know how to compute grouped summaries of the data using `pd.groupby().aggregate()`. For example, let's compute the mean temperature for each station within each month, averaged across years. Let's also compute the standard deviation and the number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0628d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe9cb7a",
   "metadata": {},
   "source": [
    "This is handy information, and it's convenient to be able to easily collect it in a summary table. However, there are some cases in which we may wish to compute new columns without creating a smaller summary table. Here's an example: \n",
    "\n",
    "## Temperature Anomaly Detection\n",
    "\n",
    "Suppose we'd like to construct a list of unusually hot or cold months in our data set. For example, if February in 1995 is much warmer than average, we'd like to detect this. What makes a month \"unusually hot or cold\"? There are lots of valid ways to define this. How would you approach this? \n",
    "\n",
    "<br> <br> <br> <br> <br> \n",
    "\n",
    "For our first attempt, let's ask the following idea: \n",
    "\n",
    "> For each temperature reading, how does that reading compare to the average reading *in that month* and *at that measurement station*?\n",
    "\n",
    "For example, if July in 2017 at Amundsen-Scott station was much warmer than the average July reading at that station, then we might say that July 2017 was anomalous. \n",
    "\n",
    "### Z-Scores\n",
    "\n",
    "To make this concrete, let's say that a given month in a year is anomalous if it is more than two standard deviations away from the mean for that month. If you've taken a statistics class, this is the same as requiring that the *z-score* for that month is larger than 2 in absolute value. That is, we should compute:\n",
    "\n",
    "$$z = \\frac{\\text{reading} - \\text{average reading at station in month}}{\\text{standard deviation at station in month}}$$\n",
    "\n",
    "and ask whether $|z| > 2$. \n",
    "\n",
    "How to compute this? Well, we already know how to compute means and standard deviations using methods like our table above, but it's hard to make comparisons to individual months this way. Can you think of how you would perform such a computation in Python? \n",
    "\n",
    "<br> <br> <br> <br> <br> \n",
    "\n",
    "If you suggested that we `merge` the summary table from above to our original `df`, that would eventually work! But `merge` is a slow operation, and we can actually avoid it by using what are sometimes called *window functions.* A window function operates on grouped data, **without reducing the length of the data frame.** In `pandas`, the most general way to create window functions is by using the `transform()` method of data frames and series. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd193e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average temperature in each month for each station\n",
    "# notice the length of the result! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa1178",
   "metadata": {},
   "source": [
    "Compare this to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2083bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the length! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8febcb4",
   "metadata": {},
   "source": [
    "Because the length of the output of `transform` is the same as that of the original data, we can use `transform` to create new columns. Here's a simple function to compute z-scores of an array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115934b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e956f034",
   "metadata": {},
   "source": [
    "Now we can compute the z-scores in one shot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d70a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ffd017b",
   "metadata": {},
   "source": [
    "Using `transform`, we can skip both computing the summary table and merging it later. We're now ready to find anomalous months in our data. Before we do, we're going to add a handy column to the original data for plotting purposes. We already saw this code in a [prior lecture](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16B/blob/master/lectures/EDA/pd-1.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff7e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "721b8cd5",
   "metadata": {},
   "source": [
    "Ok, now let's get a subset data frame with the temperature anomalies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84062a0",
   "metadata": {},
   "source": [
    "We can now, for example, plot these anomalies for a given station: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae9a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799b7a97",
   "metadata": {},
   "source": [
    "It looks like the rate of anomalies is increasing with time -- yikes. Predictably, most of the anomalies are anomalously *warm*. \n",
    "\n",
    "### Max and Min (Optional)\n",
    "\n",
    "That approach works fine, but suppose now that we'd like to try things a different approach: we want to compute the warmest and coldest instances of each month on record. For example, we'd like to answer questions like: \n",
    "\n",
    "> *In what year did Amundsen-Scott Station record the warmest February, on average?* \n",
    "\n",
    "One approach to this is to define a function to compute rankings on individual subsets, and invoke it using `transform()`. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to look up np.argsort() and think about why this works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new function to create rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b46060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coldest months on record\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f952243",
   "metadata": {},
   "source": [
    "There's a bit of a sticky point here: it's easy to get the *coldest* months on record this way, because they all have the same rank (0). However, getting the *warmest* months is a little trickier, because they have different ranks (due to some stations not having data in all years or months): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99347401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de100c23",
   "metadata": {},
   "source": [
    "How can we extract the warmest months? We can use `transform()` again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fdc510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe374b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8622055f",
   "metadata": {},
   "source": [
    "## Other Approaches\n",
    "\n",
    "Many tasks in `pandas` can be performed in more than one way. In this lecture so far, I've focused on the `transform` method of grouped data frames, which is extraordinarily flexible and can be used for many purposes. However, there are also specialized methods that you may wish to research on your own time. For example, Pandas offers a [dedicated ranking method](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.rank.html), as well as a [dedicated filtering](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html) method. If you find yourself needing to perform many ranking or filtering operations, learning these methods may be a good use of your time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c953c",
   "metadata": {},
   "source": [
    "# Custom Aggregation\n",
    "\n",
    "Earlier in this lecture, we reviewed an example of how to compute aggregates like means and standard deviations using `aggregate`. Here it is again: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406109a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09c8d15a",
   "metadata": {},
   "source": [
    "That's fun, but we can also compute *custom aggregates* using any function we want that takes in a series of numbers and spits out a new number. This is a very powerful ability, especially if you try get a little creative with it! The `apply` method is usually the way to go. For example, let's compute a simple estimate of the **year-over-year average change in temperature** in each month at each station. For this, we'll use our old friend, linear regression. We'll use the statistical fact that, when regressing `Temp` against `Year`, the coefficient of `Year` will be an estimate of the yearly change in `Temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b3187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e89ac92",
   "metadata": {},
   "source": [
    "Although this might look a bit strange as a function for using `apply` (wasn't this from the machine learning part of the class?), it's a perfectly good way to compute data summaries, as it takes in two data columns and spits out a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebbb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45db2f52",
   "metadata": {},
   "source": [
    "At what proportion of station/months is the temperature rising? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c94b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d66cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce7bdb1",
   "metadata": {},
   "source": [
    "### Takeaways for Today\n",
    "\n",
    "1. The `transform()` and `apply()` methods can enable advanced computation using simple code. \n",
    "2. Pandas also offers specialized functions for common tasks, but getting the hang of `transform()` and `apply()` will take you far. \n",
    "3. Global warming is scary. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PIC16B] *",
   "language": "python",
   "name": "conda-env-PIC16B-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
