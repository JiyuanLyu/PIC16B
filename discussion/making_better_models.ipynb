{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Better Models\n",
    "\n",
    "*This worksheet was originally designed by [Erin George](https://www.math.ucla.edu/~egeo/) (Department of Mathematics, UCLA). It has been subsequently revised by later TAs and instructors.*\n",
    "\n",
    "Before, I've supplied data and a model, and let you all mess with the model to try to improve.  This is great practice!  This time, let's start with a model and try to improve it together.  Hopefully you can see some new techniques this way.\n",
    "\n",
    "Let's use the [Fortune 1000](https://www.kaggle.com/winston56/fortune-500-data-2021) dataset from Kaggle.  I've put it on CCLE as well.  When using real-world datasets like this one, we aren't going to get perfect models.  This is just a reality with machine learning.  This is especially true with datasets that capture complex phenonmena, such as social science or economics datasets.  Still, there's no harm in trying.  Let's see how good we can make it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('Fortune_1000.csv')\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset needs a lot of cleaning and preprocessing.  Let's only do what is necessary to produce a working model at first.  Then we can see if there's anything else we need to do.\n",
    "\n",
    "First, let's remove the columns that we wouldn't want to use as part of our models.  These are going to be the columns that contain very specific and not generalizeable information, such as name and CEO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['company', 'city', 'CEO', 'Website', 'Ticker'], inplace=True)\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, when we read the information for the dataset, we see that a lot of the information is only available for the top 500 companies.  So we will either need to remove half of the data points or remove some of the features.  Which should we do?  Well, it depends on what we actually want to model.  A reasonable thing to model here would be the `rank_change` column as that sounds like something we'd actually want to predict (if we have an accurate model for this, we might be able to predict next year's rank change, for instance).  This is a feature that is only available for companies that are in the top 500 this year AND last year.  These are the companies that have 'no' for the `newcomer` column.  So let's take that.  Once we do this, the `newcomer` column will be pretty useless, so we can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['newcomer'] == 'no']\n",
    "df.drop(columns=['newcomer'], inplace=True)\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can remove the data with blanks.  It's just `Market Cap` that has missing data.  This seems like a useful column to keep, so let's remove the rows with missing data there.  Also, this column is actually stored as strings.  That's no good: let's make the values floats.  Same with `prev_rank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df[df['Market Cap'] != '-']\n",
    "df['Market Cap'] = df['Market Cap'].astype(float)\n",
    "df['prev_rank'] = df['prev_rank'].astype(float)\n",
    "\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's remove the `rank_change` columns to get our labels.  We can deduce the rank change pretty easily from `rank` and `prev_rank`, so we should remove one of those as well.  If we're trying to predict the change in rank in the future, it makes more sense to use the `prev_rank`, so we'll keep that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['rank'], inplace=True)\n",
    "y = df['rank_change']\n",
    "x = df.drop(columns=['rank_change'])\n",
    "\n",
    "display(x.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make our test and validation sets.  We'll use the `random_state` parameter of `train_test_split` to ensure the test set doesn't change from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_trv, x_test, y_trv, y_test = train_test_split(x, y, random_state = 15, test_size = 0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trv, y_trv, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing with our data.  We have categorical features.  We should encode them!  We'll use the `OneHotEncoder` from scikit-learn since it's the option that makes the most sense for our data (the other ones are mean to used on the labels or on ordered classes).  We don't need to use this on the yes/no data, we can just set that to 0 or 1 ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "x_train.loc[:,'ceo_founder'] = (x_train['ceo_founder'] == 'yes').astype(int)\n",
    "x_val.loc[:,'ceo_founder']   = (x_val['ceo_founder']   == 'yes').astype(int)\n",
    "x_test.loc[:,'ceo_founder']  = (x_test['ceo_founder']  == 'yes').astype(int)\n",
    "\n",
    "x_train.loc[:,'ceo_woman'] = (x_train['ceo_woman'] == 'yes').astype(int)\n",
    "x_val.loc[:,'ceo_woman']   = (x_val['ceo_woman']   == 'yes').astype(int)\n",
    "x_test.loc[:,'ceo_woman']  = (x_test['ceo_woman']  == 'yes').astype(int)\n",
    "\n",
    "x_train.loc[:,'profitable'] = (x_train['profitable'] == 'yes').astype(int)\n",
    "x_val.loc[:,'profitable']   = (x_val['profitable']   == 'yes').astype(int)\n",
    "x_test.loc[:,'profitable']  = (x_test['profitable']  == 'yes').astype(int)\n",
    "\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "x_val.reset_index(inplace=True, drop=True)\n",
    "x_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(x_train[['state', 'sector']])\n",
    "x_train = x_train.join(pd.DataFrame(ohe.transform(x_train[['state', 'sector']]).toarray()))\n",
    "x_val   = x_val.join(pd.DataFrame(ohe.transform(x_val[['state', 'sector']]).toarray()))\n",
    "x_test  = x_test.join(pd.DataFrame(ohe.transform(x_test[['state', 'sector']]).toarray()))\n",
    "\n",
    "x_train.drop(columns=['sector', 'state'], inplace=True)\n",
    "x_val.drop(columns=['sector', 'state'], inplace=True)\n",
    "x_test.drop(columns=['sector', 'state'], inplace=True)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make our model!  We want to predict the `rank_change`.  Is this regression or classification?  If you listen to what I said before, you would probably think it's classification.  Even though the change in rank is a numerical value, it's a discrete variable that only takes integer values.\n",
    "\n",
    "Turns out, what I said then was a little wrong, sorry!\n",
    "\n",
    "So here, even though a rank change of 4.5 would never actually occur, a rank change of \"about 4.5\" still makes a lot of sense.  Also, the changes are *ordered* while classification problems are *unordered*.  And, even though the rank changes can only be integer values, there's still an infintie number of values it can potentionally be.  In classification, we want to have a finite number of classes.\n",
    "\n",
    "That being said, we could still frame this as classification if we wanted to.  But regression makes a little bit more sense.  Here's the model we will be starting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good measure at the end to see how predict the model is to look at the $r^2$ between the predicted values and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is... not good.  How can we fix it?\n",
    "\n",
    "Well, let's rethink some of our features.  The \"state\" feature is a little suspicious.  We still have the dataset before we did any encoding.  Let's see exactly the distribution of states in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('state').aggregate('count')['rank_change']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of states that barely appear in the data.  Maybe we should reconsider things?  What could we do with this instead?\n",
    "\n",
    "Well, one option is to group up states so that there are fewer classes and more data points per class.  Maybe every state with a few number of companies becomes an \"other\" class, or we group by region of the US instead of by state.  This is actually a pretty good idea, but would be a little tricky to do.\n",
    "\n",
    "The other option is to just remove the column.  This actually isn't a bad as it sounds: a confusing data point isn't much better than no data point at all.  Let's do that.  There's not really an easy way to undo the encoding we just did, so let's just start from splitting again.  Let's also reorder some of the encoding so we don't have to keep doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.drop(columns=['state'], inplace=True)\n",
    "x.loc[:,'ceo_founder'] = (x['ceo_founder'] == 'yes').astype(int)\n",
    "x.loc[:,'ceo_woman']   = (x['ceo_woman']   == 'yes').astype(int)\n",
    "x.loc[:,'profitable']  = (x['profitable'] == 'yes').astype(int)\n",
    "\n",
    "x_trv, x_test, y_trv, y_test = train_test_split(x, y, random_state = 15, test_size = 0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trv, y_trv, test_size = 0.2)\n",
    "\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "x_val.reset_index(inplace=True, drop=True)\n",
    "x_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(x_train[['sector']])\n",
    "x_train = x_train.join(pd.DataFrame(ohe.transform(x_train[['sector']]).toarray()))\n",
    "x_val   = x_val.join(pd.DataFrame(ohe.transform(x_val[['sector']]).toarray()))\n",
    "x_test  = x_test.join(pd.DataFrame(ohe.transform(x_test[['sector']]).toarray()))\n",
    "\n",
    "x_train.drop(columns=['sector'], inplace=True)\n",
    "x_val.drop(columns=['sector'], inplace=True)\n",
    "x_test.drop(columns=['sector'], inplace=True)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make it better?  Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still pretty bad, but it looks a little better.  What else could we fix?  We never messed with the quantitative variables, maybe we should modify them a little bit?\n",
    "\n",
    "We might be tempted to standardize them.  What if we did?  Let's take a look at one of them: number of employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(x['num. of employees'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These take a LARGE range of values.  Even if we standardize them so that the mean is zero and the standard deviation is one, most of the data is going to be clumped together and there's going to be a few outliers.  Is there something better we can do?\n",
    "\n",
    "Well, a lot of data that's spread out like this is actually pretty nicely distributed on a log scale.  Let's see how our data looks in a log scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(x['num. of employees']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, that's *much* nicer.  Does it work with the other ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].hist(x['revenue'])\n",
    "axs[1].hist(np.log(x['revenue']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].hist(x['Market Cap'])\n",
    "axs[1].hist(np.log(x['Market Cap']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distributions aren't as good as the first one, but it's still much better under a log scale.\n",
    "\n",
    "Profit is another thing we should check.  Unfortunately, the profit is sometimes negative, which doens't work for log.  What can we do?\n",
    "\n",
    "Well, there's a \"made a profit?\" column, so we can take the absolute value of the profit without losing any information.  This does destroy the ordinal nature of the profit column though, so this doesn't seem ideal.\n",
    "\n",
    "Another ideal is to combine profit and revenue to create an expenses column, which would be positive numbers.  Let's try that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "axs[0].hist(x['revenue'] - x['profit'])\n",
    "axs[1].hist(np.log(x['revenue'] - x['profit']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good too!   Let's make these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['profit']            = np.log(x['revenue'] - x['profit'])\n",
    "x['num. of employees'] = np.log(x['num. of employees'])\n",
    "x['revenue']           = np.log(x['revenue'])\n",
    "x['Market Cap']        = np.log(x['Market Cap'])\n",
    "x.rename(columns = {'revenue': 'log(revenue)',\n",
    "                    'profit': 'log(expenses)',\n",
    "                    'num. of employees': 'log(employees)',\n",
    "                    'Market Cap': 'log(market cap)'}, inplace=True)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trv, x_test, y_trv, y_test = train_test_split(x, y, random_state = 15, test_size = 0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trv, y_trv, test_size = 0.2)\n",
    "\n",
    "x_train.reset_index(inplace=True, drop=True)\n",
    "x_val.reset_index(inplace=True, drop=True)\n",
    "x_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(x_train[['sector']])\n",
    "x_train = x_train.join(pd.DataFrame(ohe.transform(x_train[['sector']]).toarray()))\n",
    "x_val   = x_val.join(pd.DataFrame(ohe.transform(x_val[['sector']]).toarray()))\n",
    "x_test  = x_test.join(pd.DataFrame(ohe.transform(x_test[['sector']]).toarray()))\n",
    "\n",
    "x_train.drop(columns=['sector'], inplace=True)\n",
    "x_val.drop(columns=['sector'], inplace=True)\n",
    "x_test.drop(columns=['sector'], inplace=True)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not very good.  Let's try standardizing the quantitative features now that they are better scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s = StandardScaler()\n",
    "scale_cols = ['log(revenue)', 'log(expenses)', 'log(employees)', 'log(market cap)']\n",
    "s.fit(x_train[scale_cols])\n",
    "\n",
    "x_train.loc[:, scale_cols] = s.transform(x_train[scale_cols])\n",
    "x_val.loc[:, scale_cols]   = s.transform(x_val[scale_cols])\n",
    "x_test.loc[:, scale_cols]  = s.transform(x_test[scale_cols])\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not very good, but it is better.  We don't seem to be overfitting, so what if we trained our model more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=500, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go.  This is much better!  What if we train our model even longer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 50, activation = 'sigmoid', input_shape = (x_train.shape[1],)),\n",
    "    keras.layers.Dense(units = 10, activation = 'sigmoid'),\n",
    "    keras.layers.Dense(units = 1),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "model.fit(x_train, y_train, epochs=1000, validation_data=(x_val, y_val), verbose=0)\n",
    "\n",
    "train_mse = model.evaluate(x_train, y_train)\n",
    "train_var = y_train.var()\n",
    "\n",
    "val_mse = model.evaluate(x_val, y_val)\n",
    "val_var = y_val.var()\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train r^2: {(train_var - train_mse)/train_var}')\n",
    "\n",
    "print(f'Validation MSE: {val_mse}')\n",
    "print(f'Validation r^2: {(val_var - val_mse)/val_var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look pretty good!  We can work on this more if we have more time, but I expect at this point we'll be done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
